---
title: "Non-linear classification methods"
author: "IE 2064 Data Science"
date: "March 2018"
output:
  beamer_presentation: default
  slidy_presentation: default
  pdf_document: default
---
```{r loadlibraries, echo=FALSE, message=FALSE}
################################################################################
### R code from Applied Predictive Modeling (2013) by Kuhn and Johnson.
### Copyright 2013 Kuhn and Johnson
### Web Page: http://www.appliedpredictivemodeling.com
### Contact: Max Kuhn (mxkuhn@gmail.com)
###
### Chapter 13 Non-Linear Classification Models
###
### Required packages: AppliedPredictiveModeling, caret, doMC (optional)
###                    kernlab, klaR, lattice, latticeExtra, MASS, mda, nnet,
###                    pROC
###
### Data used: The grant application data. See the file 'CreateGrantData.R'
###
### Notes:
### 1) This code is provided without warranty.
###
### 2) This code should help the user reproduce the results in the
### text. There will be differences between this code and what is is
### the computing section. For example, the computing sections show
### how the source functions work (e.g. randomForest() or plsr()),
### which were not directly used when creating the book. Also, there may be
### syntax differences that occur over time as packages evolve. These files
### will reflect those changes.
###
### 3) In some cases, the calculations in the book were run in
### parallel. The sub-processes may reset the random number seed.
### Your results may slightly vary.
###
################################################################################

################################################################################
### Section 13.1 Nonlinear Discriminant Analysis

options(digits=4)
#load("grantData.RData")
#load("07_nonlinear.RData")
load("13_Non-Linear_Class.Rdata")
library(caret)
library(pROC)
library(kernlab)
library(klaR)
```
Nonlinear classification models
================

-  Linear classification models work through defining boundaries that were linear functions of the predictors.
-  In *quadratic discriminant models* the boundaries between classes can be quadratically curvilinear.
-  More complex boundaries, but may require fewer boundaries (because you do not need as many linear segments to define a class boundary)
-  Tends not to be useful when many variables are indicator variables (linear boundaries are adequate here)

Tradeoff
========

- Non-linear methods tends to be more difficult to interpret.
- The tend to be more accurate, especially when there are more potential features.
  -  The methods can internally identify more relationships between variables.
  -  But they cannot be inspected from the output.
- Use these when accuracy is more important that interpretability.
  -  When building decision maker intuition is not important.
  -  When the machine learning output is used as a pointer to further investigation.
  -  Note: It is unethical to make irrevocable decisions based on the output of machine learning methods that are not understood. That is a role for human judgement.

Mechanics
============

-  Use grant application data
-  Use LGOCV (Leave Group Out CV)

```{r}
lgoctrl <- trainControl(method = "LGOCV",
             summaryFunction = twoClassSummary,
             classProbs = TRUE,
             index = list(TrainSet = pre2008),
             savePredictions = TRUE)
```

QDA, RDA, MDA
============


Quadratic and regularized discriminant analysis
=============

-  In LDA, trained model minimized the total probability of misclassification.
- Predictors share a common covariatnce structure.
- Class boundaries were linear functions of the predictors.
-  For QDA, covariance structure can be class specific.
-  Class boundaries can be curvilinear.

![Bayes vs LDA vs QDA](./resources/BayesVsLDAVsQDA.png)

Requirements for QDA
===============

-  Class specific covariances increases the quantity of available data.
  - Must determine a covariance structure and boundary for every class boundary.
  - Covariance matrices must be invertable.
  - Number of predictors must be less than the number of cases within each class.
  - Predictors must not have pthological levels of collinearity (must be independendent).
  - If predictors are indicators for discrete categories, they can only be modeled as linear functions.


Regularized Discriminant Analysis
==========

- Maybe the best way to separate class boundaries is a structure between linear and quadratic surface

-  The tuning parameter, $\lambda$ flexes the covariance model between LDA ($\lambda=0$) and QDA ($\lambda=1$).

$$\tilde{\Sigma}_l(\lambda) = \lambda \Sigma_l + (1-\lambda)\Sigma$$

- The pooled covariance matrix can be varied from as observed to acting as if independent using the coefficient $\gamma$.

$$\Sigma(\gamma) = \gamma \Sigma + (1-\gamma) \sigma^2I$$

-  $\lambda$ and $\gamma$ can be tuned to determine how much you can break the LDA assumption of a common covariance structure and linear boundaries.

Mixture Discriminant Analysis
===================

-  Linear discriminant analysis assumed that each class had a distribution of predictors that were normally distributed.
-  MDA allows for subclasses that are normally distributed.
-  Number of subclasses is the level of complexity.
-  Like LDA, assume that they have the same covariance structure.
- E.g. three classes with three normal distributions with different means but same variance.

![MDA diagram](./resources/mdadescription.png)

MDA tuning
=========

-  Number of different subclasses per class needed.
-  MDA modifies $P(X|Y = C_l)$ by creating a per class mixture

$$D_l(x) \inf \sum_{k=1}^{L_l} \phi_{lk}D_{lk}(x)$$

- $L_l$ is the number of distributions for the $l$th class and $\phi_{lk}$ are the mixing proportions that are estimated during training.
- Tune using the number of distributions per class.

MDA implementation
==============

- 'mda' package included with caret.

```
set.seed(476)
mdaFit <- train(x = training[,reducedSet],
          y = training$Class,
          method = "mda",
          metric = "ROC",
          tries = 40,
          tuneGrid = expand.grid(subclasses = 1:8),
          trControl = lgoctrl)
```

MDA result - confusion matrix
=============

```{r mixturediscrimantanalysisoutput, echo=FALSE}
mdaFit

mdaFit$results <- mdaFit$results[!is.na(mdaFit$results$ROC),]
mdaFit$pred <- merge(mdaFit$pred,  mdaFit$bestTune)
mdaCM <- confusionMatrix(mdaFit, norm = "none")
mdaCM$table
```
```{r}
mdaRoc <- roc(response = mdaFit$pred$obs,
        predictor = mdaFit$pred$successful,
        levels = rev(levels(mdaFit$pred$obs)))
```

MDA - ROC AUC vs subclasses per class
=============
```{r mdaroc, echo=FALSE}
mdaRoc
update(plot(mdaFit, ylab = "ROC AUC (2008 Hold-Out Data)"))
```


Neural networks
=========

- We have used neural networks in regression where a single response (prediction) was generated.
- We can also use the neural network to make multiple outputs (e.g. a probability like output for each of $C$ classes).

Neural Networks
===========

-  We can represent $C$ potential class outcomes as $C$ binary indicator variables.
-  Then the neural network will predict $C$ different probabilities as a linear model of a set of hidden factors.
-  Each hidden factor is a linear model of another combination of hidden units.
-  The first set of hidden units is a linear model of combinations of observable parameters.
-  Number of hidden layers is the measure of complexity.


Neural Network mechanics
====================

-  Like the logistic regression, we want probabilities.
-  Sigmoidal function (same as with logistic regression) generates a set of probabilities.
-  Create hidden units based on the observed predictors, then do the final prediction based on the hidden units.
-  There can be multiple layers of hidden units. When taken to an extreme it results in the Google inception art project.


Neural network steps
===========

- Two stage model with $K$ units at the end.
- In between the predictors and the classes are the hidden units that are linear combinations of the predictors.
- Each class is predicted by a linear combination of the hidden units that have been transformed to be between zero and one by the sigmoid function

$$\sigma(v) = 1/(1+e^{-v})$$

- The softmax transformation can be used so that the resulting output is probability-like (all units between 0 and 1, sum adds up to 1)

$$\hat{p}^*_l = \frac{e^{\hat{y}_l}}{\sum_{l=1}^C e^{\hat{y}_l}}$$

Neural network illustration
======
![Neural networks](./resources/neural_c.png)

Overfitting in neural networks
==============================

- Neural networks for classification have a significant potential for overfitting.
- Example: Google used neural networks with many layers on image data and was able to generate images out of white noise *inceptionism*

https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html


Implementation
================

- Tuned by number of units in hidden layer and the weight decay.
-  Preprocessing will center and scale the data so that each observation variable values are in terms of the mean and standard deviation of that variable.
-  SpatialSign - projection of vector onto a unit length circle (1 1-norm)

======

```
nnetGrid <- expand.grid(size = 1:10, decay = c(0, .1, 1, 2))
maxSize <- max(nnetGrid$size)

nnetFit <- train(x = training[,reducedSet],
           y = training$Class,
           method = "nnet",
           metric = "ROC",
           preProc = c("center", "scale"),
           tuneGrid = nnetGrid,
           trace = FALSE,
           maxit = 2000,
           MaxNWts = 1*(maxSize *
              (length(reducedSet) + 1) + maxSize + 1),
           trControl = lgoctrl)
```

Results for model 1 - Simple network model
===============

- Grant model

```{r nnet1results, echo=FALSE}
nnetFit
```

=======

```
set.seed(476)
nnetFit2 <- train(x = training[,reducedSet],
              y = training$Class,
              method = "nnet",
              metric = "ROC",
              preProc = c("center", "scale", "spatialSign"),
              tuneGrid = nnetGrid,
              trace = FALSE,
              maxit = 2000,
              MaxNWts = 1*(maxSize *
                  (length(reducedSet) + 1) + maxSize + 1),
              trControl = lgoctrl)
```

Second model - Add additional preprocessing (SpatialSign)
============
```{r nnet2, echo=FALSE}
nnetFit2$results
```

===========
```
nnetGrid$bag <- FALSE

set.seed(476)
nnetFit3 <- train(x = training[,reducedSet],
              y = training$Class,
              method = "avNNet",
              metric = "ROC",
              preProc = c("center", "scale"),
              tuneGrid = nnetGrid,
              repeats = 10,
              trace = FALSE,
              maxit = 2000,
              MaxNWts = 10*(maxSize * (length(reducedSet) + 1) + maxSize + 1),
              allowParallel = FALSE,
              ## this will cause to many workers to be launched.
              trControl = lgoctrl)
```

Neural net model 3 - Use AvNNet package implementation.
===========
```{r nnet3, echo=FALSE}
nnetFit3
```

==================
```
set.seed(476)
nnetFit4 <- train(x = training[,reducedSet],
              y = training$Class,
              method = "avNNet",
              metric = "ROC",
              preProc = c("center", "scale", "spatialSign"),
              tuneGrid = nnetGrid,
              trace = FALSE,
              maxit = 2000,
              repeats = 10,
              MaxNWts = 10*(maxSize *
                (length(reducedSet) + 1) + maxSize + 1),
              allowParallel = FALSE,
              trControl = lgoctrl)
```

Neural net model 4, use AvNNet implementation with additional preprocessing (SpatialSign)
====
```{r nnet4, echo=FALSE}
nnetFit4
```
=============
```{r}
nnetFit4$pred <- merge(nnetFit4$pred,  nnetFit4$bestTune)
nnetCM <- confusionMatrix(nnetFit4, norm = "none")
```

Neural net model 4 confusion matrix
=========

```{r nnet4confusionmatrix, echo=FALSE}
nnetCM$table
```

```{r}
nnetRoc <- roc(response = nnetFit4$pred$obs,
           predictor = nnetFit4$pred$successful,
           levels = rev(levels(nnetFit4$pred$obs)))


nnet1 <- nnetFit$results
nnet1$Transform <- "No Transformation"
nnet1$Model <- "Single Model"

nnet2 <- nnetFit2$results
nnet2$Transform <- "Spatial Sign"
nnet2$Model <- "Single Model"

nnet3 <- nnetFit3$results
nnet3$Transform <- "No Transformation"
nnet3$Model <- "Model Averaging"
nnet3$bag <- NULL

nnet4 <- nnetFit4$results
nnet4$Transform <- "Spatial Sign"
nnet4$Model <- "Model Averaging"
nnet4$bag <- NULL
```

Looking at all neural net model results
========
```{r nnetresults, echo=FALSE}
nnetResults <- rbind(nnet1, nnet2, nnet3, nnet4)
nnetResults$Model <- factor(as.character(nnetResults$Model),
                            levels = c("Single Model", "Model Averaging"))
```

Plot neural net model ROC
==========
```{r, echo=FALSE}
library(latticeExtra)
useOuterStrips(
  xyplot(ROC ~ size|Model*Transform,
       data = nnetResults,
       groups = decay,
       as.table = TRUE,
       type = c("p", "l", "g"),
       lty = 1,
       ylab = "ROC AUC (2008 Hold-Out Data)",
       xlab = "Number of Hidden Units",
       auto.key = list(columns = 4,
                   title = "Weight Decay",
                   cex.title = 1)))
```

ROC curve
=========
```{r, echo=FALSE}
plot(nnetRoc, type = "s", legacy.axes = TRUE, print.auc=TRUE)
```


Flexible discriminant analysis
=======================

-  We can modify linear discriminant analysis to use other regression methods to define the class boundaries
  -  e.g. lasso, ridge regression, MARS
- Result can be used to create more complex boundaries
- E.g. FDA with MARS and four terms.

![LDA vs FDA (with MARS)](./resources/LDAVsFDA.png)


Application of FDA to grant model
====================

-  Tuning based the type of linear model. (e.g. retained terms for lasso or MARS)
-  Result

```
set.seed(476)
fdaFit <- train(x = training[,reducedSet],
          y = training$Class,
          method = "fda",
          metric = "ROC",
          tuneGrid = expand.grid(degree = 1, nprune = 2:25),
          trControl = lgoctrl)
```

FDA Result
============
```{r, echo=FALSE}
fdaFit
```

Plot ROC curve
=======
```{r}
fdaFit$pred <- merge(fdaFit$pred,  fdaFit$bestTune)
fdaCM <- confusionMatrix(fdaFit, norm = "none")
fdaCM

fdaRoc <- roc(response = fdaFit$pred$obs,
          predictor = fdaFit$pred$successful,
          levels = rev(levels(fdaFit$pred$obs)))
```

FDA parameter tuning ROC plot
====
```{r, echo=FALSE}
update(plot(fdaFit), ylab = "ROC AUC (2008 Hold-Out Data)")
```

===

```{r, echo=FALSE}
plot(nnetRoc, type = "s", col = rgb(.2, .2, .2, .2), legacy.axes = TRUE)
```

Support Vector Machines
=======

- Similar to the regression version, generate boundaries to define classes.
- Define planes that represent the best boundaries between classes
- If data is seperable, maximize the buffer between the boundary and the data.

![SVM plane diagram](./resources/svmplane.png)

Creating a support vector
=============

- Often, the data set is not completely separable.
  - Recognize there are no perfect boundaries.
  - Planes are scored by distance from boundary to points (greater is better) and penalized for points on wrong side or too close to boundary (within the margin)

```
set.seed(201)
sigmaRangeFull <- sigest(as.matrix(training[,fullSet]))
svmRGridFull <- expand.grid(sigma =  as.vector(sigmaRangeFull)[1],
                            C = 2^(-3:4))
set.seed(476)
svmRFitFull <- train(x = training[,fullSet],
                     y = training$Class,
                     method = "svmRadial",
                     metric = "ROC",
                     preProc = c("center", "scale"),
                     tuneGrid = svmRGridFull,
                     trControl = lgoctrl)
```

Example SVM output
==========
```{r, echo=FALSE}
svmRFitFull$finalModel
```

```
set.seed(202)
sigmaRangeReduced <- sigest(as.matrix(training[,reducedSet]))
svmRGridReduced <- expand.grid(sigma = sigmaRangeReduced[1],
                               C = 2^(seq(-4, 4)))
set.seed(476)
svmRFitReduced <- train(x = training[,reducedSet],
                        y = training$Class,
                        method = "svmRadial",
                        metric = "ROC",
                        preProc = c("center", "scale"),
                        tuneGrid = svmRGridReduced,
                        trControl = lgoctrl)
```

SVM with reduced set of predictors
=============
```{r, echo=FALSE}
svmRFitReduced$finalModel
```

```
svmPGrid <-  expand.grid(degree = 1:2,
                         scale = c(0.01, .005),
                         C = 2^(seq(-6, -2, length = 10)))

set.seed(476)
svmPFitFull <- train(x = training[,fullSet],
                     y = training$Class,
                     method = "svmPoly",
                     metric = "ROC",
                     preProc = c("center", "scale"),
                     tuneGrid = svmPGrid,
                     trControl = ctrl)
```

```{r}
svmPFitFull
```

```
svmPGrid2 <-  expand.grid(degree = 1:2,
                          scale = c(0.01, .005),
                          C = 2^(seq(-6, -2, length = 10)))
set.seed(476)
svmPFitReduced <- train(x = training[,reducedSet],
                        y = training$Class,
                        method = "svmPoly",
                        metric = "ROC",
                        preProc = c("center", "scale"),
                        tuneGrid = svmPGrid2,
                        fit = FALSE,
                        trControl = lgoctrl)
```
```{r}
svmPFitReduced
```
```{r}
svmPFitReduced$pred <- merge(svmPFitReduced$pred,  svmPFitReduced$bestTune)
svmPCM <- confusionMatrix(svmPFitReduced, norm = "none")
svmPRoc <- roc(response = svmPFitReduced$pred$obs,
           predictor = svmPFitReduced$pred$successful,
           levels = rev(levels(svmPFitReduced$pred$obs)))


svmRadialResults <- rbind(svmRFitReduced$results,
                          svmRFitFull$results)
svmRadialResults$Set <- c(rep("Reduced Set", nrow(svmRFitReduced$result)),
                          rep("Full Set", nrow(svmRFitFull$result)))
svmRadialResults$Sigma <- paste("sigma = ",
                                format(svmRadialResults$sigma,
                                       scientific = FALSE, digits= 5))
svmRadialResults <- svmRadialResults[!is.na(svmRadialResults$ROC),]
```

SVM ROC with a few different SVM models
====================

```{r, echo=FALSE}
xyplot(ROC ~ C|Set, data = svmRadialResults,
       groups = Sigma, type = c("g", "o"),
       xlab = "Cost",
       ylab = "ROC (2008 Hold-Out Data)",
       auto.key = list(columns = 2),
       scales = list(x = list(log = 2)))
```

```{r}
svmPolyResults <- rbind(svmPFitReduced$results,
                        svmPFitFull$results)
svmPolyResults$Set <- c(rep("Reduced Set", nrow(svmPFitReduced$result)),
                        rep("Full Set", nrow(svmPFitFull$result)))
svmPolyResults <- svmPolyResults[!is.na(svmPolyResults$ROC),]
svmPolyResults$scale <- paste("scale = ",
                              format(svmPolyResults$scale,
                                     scientific = FALSE))
svmPolyResults$Degree <- "Linear"
svmPolyResults$Degree[svmPolyResults$degree == 2] <- "Quadratic"
useOuterStrips(xyplot(ROC ~ C|Degree*Set, data = svmPolyResults,
                      groups = scale, type = c("g", "o"),
                      xlab = "Cost",
                      ylab = "ROC (2008 Hold-Out Data)",
                      auto.key = list(columns = 2),
                      scales = list(x = list(log = 2))))
```

Neural Net vs. FDA vs. SVM
==========
```{r}
plot(nnetRoc, type = "s", col = rgb(.8, .2, .2, .2), legacy.axes = TRUE)
plot(fdaRoc, type = "s", add = TRUE, col = rgb(.2, .8, .2, .2), legacy.axes = TRUE)
plot(svmPRoc, type = "s", add = TRUE, col = rgb(.2, .2, .8, .2), legacy.axes = TRUE)
```

K-Nearest Neighbors
====

-  For each observation, make prediction based on polling the nearest $N$ trained data points.
-  'nearest' needs to be defined. Usually use Euclidean metrics (square root of the sum of the squares of differences in predictors)
-  Typically, you would center and scale predictors first.


Metrics
========

-  *Nearest* is defined by some metric.
-  Euclidean

$$\left(\sum_{j=1}^P (x_{aj} - x_{bj})^2)\right)^{1/2}$$

- Minkowiski - A generalization of the Euclidean geometry for $q>0$.

$$\left(\sum_{j=1}^P |x_{aj} - x_{bj}|^q\right)^{1/q}$$

- Manhattan distance - Minkowski distance with $q=1$.

$$\left(\sum_{j=1}^P |x_{aj} - x_{bj}|\right)$$

- Other metrics can be used depending on the setting.

- With categorical features, treat like values of having a difference of 0, and unlike values of having a difference of 1.


kNN quality
============

- Can be poor when the local predictor structure is not relevant to the response.
- If some predictors are irrelevent or noisy.
  - Pre-processing can remove irrelevent predictors.
- May need weighing of predictors or of samples used to predict the response.
Implementation of kNN
=========

```{r}
set.seed(476)
knnFit <- train(x = training[,reducedSet],
                y = training$Class,
                method = "knn",
                metric = "ROC",
                preProc = c("center", "scale"),
                tuneGrid = data.frame(k = c(4*(0:5)+1,20*(1:5)+1,50*(2:9)+1)),
                trControl = ctrl)
```

kNN output
=============
```{r, echo=FALSE}
knnFit$finalModel
```

KNN Confusion matrix
=====
```{r}
knnFit$pred <- merge(knnFit$pred,  knnFit$bestTune)
knnCM <- confusionMatrix(knnFit, norm = "none")
```
```{r, echo=FALSE}
knnCM$table
```
KNN ROC plot
===========
```{r}
knnRoc <- roc(response = knnFit$pred$obs,
              predictor = knnFit$pred$successful,
              levels = rev(levels(knnFit$pred$obs)))
```
```{r, echo=FALSE}
update(plot(knnFit, ylab = "ROC (2008 Hold-Out Data)"))
```

Comparisons plot of FDA, Neural Net, SVM, KNN
=====
```{r, echo=FALSE, message=FALSE}
plot(fdaRoc, type = "s", col = rgb(.8, .2, .2, .2), legacy.axes = TRUE)
plot(nnetRoc, type = "s", add = TRUE, col = rgb(.2, .8, .2, .2), legacy.axes = TRUE)
plot(svmPRoc, type = "s", add = TRUE, col = rgb(.2, .2, .8, .2), legacy.axes = TRUE)
plot(knnRoc, type = "s", add = TRUE, legacy.axes = TRUE)
```


Naive Bayes
=====

- Bayes rule - Based on the predictors observed, what is the conditional probability of the outcome.

$$P(Y=C_{l}|X)= \frac{P(Y) P(X|Y=C_{l})}{P(X)}$$

-  $P(Y)$ is the prior probability
-  $P(X)$ is the probability of the predictor values
-  $P(X|Y=C_l)$ is the conditional probability
-  But $Y$ can be very complex (conditioning on many variables)

Naive Bayes - Simplifying Bayes rule
=============

- Naive Bayes simplifies Bayes rule by assuming all predictors are independent of the others.

$$P(X|Y=C_l) = \Pi_{j=1}^P P(X_j|Y=C_l)$$

-  So instead of calculating joint probabilities, multiply conditionals.
-  Assumption is false, but is often close enough for a good predictor.
-  We need to make a check along the way for issues such as conditional observed frequencies = 0
- Can introduce a *Laplace  correction factor* between 1 and 2 in the training data in each class to the numerator and denominator.

```{r}
factors <- c("SponsorCode", "ContractValueBand", "Month", "Weekday")
nbPredictors <- factorPredictors[factorPredictors %in% reducedSet]
#nbPredictors <- c(nbPredictors, factors)
nbPredictors <- nbPredictors[nbPredictors != "SponsorUnk"]
```
```
nbTraining <- training[, c("Class", nbPredictors)]
nbTesting <- testing[, c("Class", nbPredictors)]

for(i in nbPredictors)
{
  if(length(unique(training[,i])) <= 15)
  {
    nbTraining[, i] <- factor(nbTraining[,i], levels = paste(sort(unique(training[,i]))))
    nbTesting[, i] <- factor(nbTesting[,i], levels = paste(sort(unique(training[,i]))))
  }
}

set.seed(476)
nBayesFit <- train(x = nbTraining[,nbPredictors],
                   y = nbTraining$Class,
                   method = "nb",
                   metric = "ROC",
                   tuneGrid = data.frame(usekernel = c(TRUE, FALSE), fL = 2, adjust = c(TRUE, FALSE)),
                   trControl = ctrl)
```

Naive Bayes model
==============
```{r, echo=FALSE}
nBayesFit$finalModel$apriori
```

Naive Bayes confusion matrix
==============
```{r, echo=FALSE}
nBayesFit$pred <- merge(nBayesFit$pred,  nBayesFit$bestTune)
nBayesCM <- confusionMatrix(nBayesFit, norm = "none")
nBayesCM$table
```

```{r, echo=FALSE}
nBayesRoc <- roc(response = nBayesFit$pred$obs,
                 predictor = nBayesFit$pred$successful,
                 levels = rev(levels(nBayesFit$pred$obs)))
```

Neural Net vs. Naive Bayes ROC
=====
```{r, echo=FALSE, message=FALSE, warning=FALSE}
par(mfrow=c(2,1))
plot(nnetRoc, type = "s", col = rgb(.2, .2, .2, .2), legacy.axes = TRUE, print.auc=TRUE )

plot(nBayesRoc, type = "s", legacy.axes = TRUE, print.auc=TRUE)
par(mfrow=c(1,1))
```
