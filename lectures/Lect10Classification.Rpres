```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```
Classification
========================================================
author: IE2064 Data Science
date: February 2015



Classification
=============================

-  Logistics Regression
-  Naive Bayes
-  *Adaboost* classifiers
-  Random Forests
-  *k*-Nearest Neighbors

```{r salesdata, echo=FALSE}
library(DMwR)
data(sales)
```


```{r, echo=FALSE}
totS <- table(sales$ID)
totP <- table(sales$Prod)
```

```{r, echo=FALSE}
sales$Uprice <- sales$Val/sales$Quant
```

```{r, echo=FALSE}
totS <- table(sales$ID)
totP <- table(sales$Prod)
```

```{r, echo=FALSE}
sales1 <- sales[-which(is.na(sales$Quant) & is.na(sales$Val)),]
sales2 <- sales1[!sales1$Prod %in% c('p2442','p2443'),]
sales2$Prod <- factor(sales2$Prod)
```


```{r, echo=FALSE}
tPrice <- tapply(sales2[sales2$Insp != 'fraud','Uprice'],list(sales2[sales2$Insp != 'fraud','Prod']),median,na.rm=T)
```

```{r, echo=FALSE}
noQuant <- which(is.na(sales2$Quant))
sales2[noQuant,'Quant'] <- ceiling(sales2[noQuant,'Val'] /
                       tPrice[sales2[noQuant,'Prod']])
noVal <- which(is.na(sales2$Val))
sales2[noVal,'Val'] <- sales2[noVal,'Quant'] *
                   tPrice[sales2[noVal,'Prod']]
sales2$Uprice <- sales2$Val/sales2$Quant
```

```{r, echo=FALSE, warning=FALSE}
avgNDTP <- function(toInsp,train,stats) {
  if (missing(train) && missing(stats)) 
    stop('Provide either the training data or the product stats')
  if (missing(stats)) {
    notF <- which(train$Insp != 'fraud')
    stats <- tapply(train$Uprice[notF],
                    list(Prod=train$Prod[notF]),
                    function(x) {
                      bp <- boxplot.stats(x)$stats
                      c(median=bp[3],iqr=bp[4]-bp[2])
                    }) 
    stats <- matrix(unlist(stats),
                    length(stats),2,byrow=T,
                    dimnames=list(names(stats),c('median','iqr')))
    stats[which(stats[,'iqr']==0),'iqr'] <- 
        stats[which(stats[,'iqr']==0),'median']
  }

  mdtp <- mean(abs(toInsp$Uprice-stats[toInsp$Prod,'median']) /
               stats[toInsp$Prod,'iqr'])
  return(mdtp)
}
evalOutlierRanking <- function(testSet,rankOrder,Threshold,statsProds) {
  ordTS <- testSet[rankOrder,]
  N <- nrow(testSet)
  nF <- if (Threshold < 1) as.integer(Threshold*N) else Threshold
  cm <- table(c(rep('fraud',nF),rep('ok',N-nF)),ordTS$Insp)
  prec <- cm['fraud','fraud']/sum(cm['fraud',])
  rec <- cm['fraud','fraud']/sum(cm[,'fraud'])
  AVGndtp <- avgNDTP(ordTS[nF,],stats=statsProds)
  return(c(Precision=prec,Recall=rec,avgNDTP=AVGndtp))
}

BPrule <- function(train,test) {
  notF <- which(train$Insp != 'fraud')
  ms <- tapply(train$Uprice[notF],
               list(Prod=train$Prod[notF]),
               function(x) {
                 bp <- boxplot.stats(x)$stats
                 c(median=bp[3],iqr=bp[4]-bp[2])
               })
  ms <- matrix(unlist(ms),length(ms),2,byrow=T,
               dimnames=list(names(ms),c('median','iqr')))
  ms[which(ms[,'iqr']==0),'iqr'] <- ms[which(ms[,'iqr']==0),'median']
  ORscore <- abs(test$Uprice-ms[test$Prod,'median']) /
             ms[test$Prod,'iqr']
  return(list(rankOrder=order(ORscore,decreasing=T),
              rankScore=ORscore))
}


notF <- which(sales$Insp != 'fraud')
globalStats <- tapply(sales$Uprice[notF],
                      list(Prod=sales$Prod[notF]),
                      function(x) {
                        bp <- boxplot.stats(x)$stats
                        c(median=bp[3],iqr=bp[4]-bp[2])
                      })
globalStats <- matrix(unlist(globalStats),
                length(globalStats),2,byrow=T,
                dimnames=list(names(globalStats),c('median','iqr')))
globalStats[which(globalStats[,'iqr']==0),'iqr'] <- 
   globalStats[which(globalStats[,'iqr']==0),'median']
```
Classification - Supervised approaches
=======================================
type: section

-  Clustering based on using labeled data.

Class imbalance problem
========================

-  One problem in classification is that fraudulent reports are a clear minority.
  -  8.1% of inspected reports.
  -  Deciding all reports are valid gives 91% accuracy!
  -  The minority class may be disregarded.
  -  But this is the group we are most interested in!
  
Techniques for working with class imbalance
===========================================

1.  Method that bias the learning process by using evaluation metrics that are more sensitive to minority class examples.
2.  Sampling methods that manipulate the training data to change the class distribution.

Sampling methods that manipulate training data
==============================================

-  *Under-sampling*:  select a small part of the majority class examples to add to the minority class cases.
-  *Over-sampling*: use some process to replicate the minority class examples.

Synthetic Minority Oversampling Technique (SMOTE)
=====================================================

-  Artificially generates new examples of the minority class using the nearest neighbors of these cases. 
-  Undersample majority class so that the dataset is more balanced.

SMOTE with iris data
====================

```{r}
data(iris)
data <- iris[,c(1,2,5)]
data$Species <- factor(ifelse(data$Species == 'setosa','rare','common'))
newData <- SMOTE(Species ~ .,data,perc.over=600)
```
-  Create six new cases for every member of the minority class.
-  Done through random interpolation between the case and its nearest neighbors.

SMOTE creates new cases
=======================
```{r}
table(data$Species)
table(newData$Species)
```

-  Use this training data for supervised learning techniques.

Visualizing SMOTE
=================
```{r, echo=FALSE}
par(mfrow=c(1,2))
plot(data[,1],data[,2],pch=19+as.integer(data[,3]),main='Original Data')
plot(newData[,1],newData[,2],pch=19+as.integer(newData[,3]),main="SMOTE'd Data")
```

Naive Bayes
============
type: sub-section

- *Naive Bayes* is a probabilistic classifier based on Bayes theorem.
-  Assumes independence between predictors, hence *naive*
-  Still fairly successful.
$$P(c|X_1, \ldots, X_p) = \frac{P(c)P(P(X_1, \ldots, X_p|c)}{P(X_1, \ldots, X_p)}$$

Implementation of Naive Bayes
=============================

-  Assume independence to simplify the numerator
$$P(c|X_1, \ldots, X_p) = \frac{P(c)\Pi_{i=1}^p P(X_i|c)}{P(X_1, \ldots, X_p)}$$
-  Estimate probabilities using the labeled test data.

R Implementations
===================

-  Naive Bayes is implemented in many packages.
-  Requires you specify the:
  -  Model
  -  Training data
  -  Predictors
-  Evaluation
  -  Based on hold-out routines

Using Naive Bayes
=================
```{r}
nb <- function(train,test) {
  require(e1071,quietly=T)
  sup <- which(train$Insp != 'unkn')
  data <- train[sup,c('ID','Prod','Uprice','Insp')]
  data$Insp <- factor(data$Insp,levels=c('ok','fraud'))
  model <- naiveBayes(Insp ~ .,data)
  preds <- predict(model,test[,c('ID','Prod','Uprice','Insp')],type='raw')
  return(list(rankOrder=order(preds[,'fraud'],decreasing=T),
              rankScore=preds[,'fraud'])
         )
}
```

Evaluation of Naive Bayes
==========================
```{r}
ho.nb <- function(form, train, test, ...) {
  res <- nb(train,test)
  structure(evalOutlierRanking(test,res$rankOrder,...),
            itInfo=list(preds=res$rankScore,
                        trues=ifelse(test$Insp=='fraud',1,0)
                       )
           )
}
```
```{r, echo=FALSE}
nb.res <- holdOut(learner('ho.nb',
                          pars=list(Threshold=0.1,
                                    statsProds=globalStats)),
                  dataset(Insp ~ .,sales),
                  hldSettings(3,0.3,1234,T),
                  itsInfo=TRUE
                  )
```

Results
========


```{r}
summary(nb.res)
```


```{r, echo=FALSE}
ho.ORh <- function(form, train, test, ...) {
  #require(dprep,quietly=T)
  ntr <- nrow(train)
  all <- rbind(train,test)
  N <- nrow(all)
  ups <- split(all$Uprice,all$Prod)
  r <- list(length=ups)
  for(u in seq(along=ups)) 
    r[[u]] <- if (NROW(ups[[u]]) > 3) 
                 outliers.ranking(ups[[u]])$prob.outliers 
              else if (NROW(ups[[u]])) rep(0,NROW(ups[[u]])) 
              else NULL
  all$lof <- vector(length=N)
  split(all$lof,all$Prod) <- r
  all$lof[which(!(is.infinite(all$lof) | is.nan(all$lof)))] <- 
    SoftMax(all$lof[which(!(is.infinite(all$lof) | is.nan(all$lof)))])
  structure(evalOutlierRanking(test,order(all[(ntr+1):N,'lof'],
                                           decreasing=T),...),
            itInfo=list(preds=all[(ntr+1):N,'lof'],
                        trues=ifelse(test$Insp=='fraud',1,0))
           )
}


orh.res <- holdOut(learner('ho.ORh',
                          pars=list(Threshold=0.1,
                                    statsProds=globalStats)),
                  dataset(Insp ~ .,sales),
                  hldSettings(3,0.3,1234,T),
                  itsInfo=TRUE
                  )
info <- attr(orh.res,'itsInfo')
PTs.orh <- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)),
                 c(1,3,2)
                )
```
Now, compare against ORh
========================

```{r, echo=FALSE}
par(mfrow=c(1,2))
info <- attr(nb.res,'itsInfo')
PTs.nb <- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)),
                 c(1,3,2)
                )
PRcurve(PTs.nb[,,1],PTs.nb[,,2],
        main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1),
        avg='vertical')
PRcurve(PTs.orh[,,1],PTs.orh[,,2],
        add=T,lty=1,col='grey',
        avg='vertical')        
legend('topright',c('NaiveBayes','ORh'),
       lty=1,col=c('black','grey'))
CRchart(PTs.nb[,,1],PTs.nb[,,2],
        main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1),
        avg='vertical')
CRchart(PTs.orh[,,1],PTs.orh[,,2],
        add=T,lty=1,col='grey',
        avg='vertical')        
legend('bottomright',c('NaiveBayes','ORh'),
       lty=1,col=c('black','grey'))
```

Why is it worse?
=================

Why is it worse?
================

-  Class imbalance?
-  Try to apply SMOTE

```{r, echo=FALSE}
nb.s <- function(train,test) {
  require(e1071,quietly=T)
  sup <- which(train$Insp != 'unkn')
  data <- train[sup,c('ID','Prod','Uprice','Insp')]
  data$Insp <- factor(data$Insp,levels=c('ok','fraud'))
  newData <- SMOTE(Insp ~ .,data,perc.over=700)
  model <- naiveBayes(Insp ~ .,newData)
  preds <- predict(model,test[,c('ID','Prod','Uprice','Insp')],type='raw')
  return(list(rankOrder=order(preds[,'fraud'],decreasing=T),
              rankScore=preds[,'fraud'])
         )
}


ho.nbs <- function(form, train, test, ...) {
  res <- nb.s(train,test)
  structure(evalOutlierRanking(test,res$rankOrder,...),
            itInfo=list(preds=res$rankScore,
                        trues=ifelse(test$Insp=='fraud',1,0)
                       )
           )
}
nbs.res <- holdOut(learner('ho.nbs',
                          pars=list(Threshold=0.1,
                                    statsProds=globalStats)),
                  dataset(Insp ~ .,sales),
                  hldSettings(3,0.3,1234,T),
                  itsInfo=TRUE
                  )
```


Results with SMOTE
==================
```{r}
summary(nbs.res)
```

Comparison plot
===============
```{r, echo=FALSE}
par(mfrow=c(1,2))
info <- attr(nbs.res,'itsInfo')
PTs.nbs <- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)),
                 c(1,3,2)
                )
PRcurve(PTs.nb[,,1],PTs.nb[,,2],
        main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1),
        avg='vertical')
PRcurve(PTs.nbs[,,1],PTs.nbs[,,2],
        add=T,lty=2,
        avg='vertical')
PRcurve(PTs.orh[,,1],PTs.orh[,,2],
        add=T,lty=1,col='grey',
        avg='vertical')        
legend('topright',c('NaiveBayes','smoteNaiveBayes','ORh'),
       lty=c(1,2,1),col=c('black','black','grey'))
CRchart(PTs.nb[,,1],PTs.nb[,,2],
        main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1),
        avg='vertical')
CRchart(PTs.nbs[,,1],PTs.nbs[,,2],
        add=T,lty=2,
        avg='vertical')
CRchart(PTs.orh[,,1],PTs.orh[,,2],
        add=T,lty=1,col='grey',
        avg='vertical')        
legend('bottomright',c('NaiveBayes','smoteNaiveBayes','ORh'),
       lty=c(1,2,1),col=c('black','black','grey'))
```

Notes on Naive Bayes
=====================

-  Does not seem to work well here.
-  In the unsupervised methods, we split the model construction by product, but we do not have as much inspected data to work with here.

AdaBoost
==========

-  Learning algorithm that is part of the class of *ensemble models*.
-  *Ensemble models* are those that are formed by a set of base models that contribute to the final prediction.
-  *Boosting* is a general method that improves performance of a base that is better than a random classifier.
-  *AdaBoost* uses adaptive boosting to obtain the set of base models.

AdaBoost development
=====================

- Base models are generated sequentially.
- Each round weighs data by increasing weights on data that was incorrectly classified by the previous model.
- Effectively creates new training data for each round.
-  Since later rounds are forced to learn how to handle cases that were misclassified earlier, the resulting ensemble should be more accurate with them.

AdaBoost example
================
```{r, echo=FALSE}
library(RWeka)
data(iris)
idx<-sample(150, 100)
model <- AdaBoostM1(Species ~ ., iris[idx,],
                    control=Weka_control(I=100))
preds <- predict(model, iris[-idx,])
table(preds, iris[-idx, 'Species'])
```

And obtain probabilistic predictions
====================================
```{r}
prob.preds <- predict(model,iris[-idx,], type='probability')
head(prob.preds)
```

Apply to transactions set
=========================
```{r, echo=FALSE}

ab <- function(train,test) {
  require(RWeka,quietly=T)
  sup <- which(train$Insp != 'unkn')
  data <- train[sup,c('ID','Prod','Uprice','Insp')]
  data$Insp <- factor(data$Insp,levels=c('ok','fraud'))
  model <- AdaBoostM1(Insp ~ .,data,
                      control=Weka_control(I=100))
  preds <- predict(model,test[,c('ID','Prod','Uprice','Insp')],
                   type='probability')
  return(list(rankOrder=order(preds[,'fraud'],decreasing=T),
              rankScore=preds[,'fraud'])
         )
}


ho.ab <- function(form, train, test, ...) {
  res <- ab(train,test)
  structure(evalOutlierRanking(test,res$rankOrder,...),
            itInfo=list(preds=res$rankScore,
                        trues=ifelse(test$Insp=='fraud',1,0)
                       )
           )
}


ab.res <- holdOut(learner('ho.ab',
                          pars=list(Threshold=0.1,
                                    statsProds=globalStats)),
                  dataset(Insp ~ .,sales),
                  hldSettings(3,0.3,1234,T),
                  itsInfo=TRUE
                  )
```
Look at the result summary
==========================
```{r}
summary(ab.res)
```

And plot AdaBoost results
=========================
```{r, echo=FALSE}
par(mfrow=c(1,2))
info <- attr(ab.res,'itsInfo')
PTs.ab <- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)),
                c(1,3,2)
                )
PRcurve(PTs.nb[,,1],PTs.nb[,,2],
        main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1),
        avg='vertical')
PRcurve(PTs.orh[,,1],PTs.orh[,,2],
        add=T,lty=1,col='grey',
        avg='vertical')        
PRcurve(PTs.ab[,,1],PTs.ab[,,2],
        add=T,lty=2,
        avg='vertical')        
legend('topright',c('NaiveBayes','ORh','AdaBoostM1'),
       lty=c(1,1,2),col=c('black','grey','black'))
CRchart(PTs.nb[,,1],PTs.nb[,,2],
        main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1),
        avg='vertical')
CRchart(PTs.orh[,,1],PTs.orh[,,2],
        add=T,lty=1,col='grey',
        avg='vertical')        
CRchart(PTs.ab[,,1],PTs.ab[,,2],
        add=T,lty=2,
        avg='vertical')        
legend('bottomright',c('NaiveBayes','ORh','AdaBoostM1'),
       lty=c(1,1,2),col=c('black','grey','black'))
```


Semi-supervised Approaches
===========================
type: section

-  Attempt to use both inspected and non-inspected reports to obtain a classification model.

Self-training
==================
-  *Self-training* builds an initial classifier using the given labeled cases.
-  Used to predict the labels of the unlabeled cases in the training case.
-  Highest confidence label is added to the labeled case, then repeat until some stopping criteria is met.

Self-training inputs
=====================
1.  Base learner.
2.  Threshold on the confidence of classifications that determines which cases are added to the new training set.
3.  Stopping criteria.

Iris example with Naive Bayes
=============================
First with labeled data
```{r, echo=FALSE}
set.seed(1234) # Just to ensrure you get the same results as in the book
library(DMwR)
library(e1071)
data(iris)
idx <- sample(150,100)
tr <- iris[idx,]
ts <- iris[-idx,]
```
```{r}
nb <- naiveBayes(Species ~ .,tr)
table(predict(nb,ts),ts$Species)
```

Now, make 90% unlabeled
=======================
```{r, echo=FALSE}
trST <- tr
nas <- sample(100,90)
trST[nas,'Species'] <- NA
func <- function(m,d) {
   p <- predict(m,d,type='raw')
   data.frame(cl=colnames(p)[apply(p,1,which.max)],p=apply(p,1,max))
}
```
```{r}
nbSTbase <- naiveBayes(Species ~ .,trST[-nas,])
table(predict(nbSTbase,ts),ts$Species)
```

Now, use self-training
======================
```{r}
nbST <- SelfTrain(Species ~ .,trST,learner('naiveBayes',list()),'func')
table(predict(nbST,ts),ts$Species)
```

```{r, echo=FALSE}
pred.nb <- function(m,d) {
  p <- predict(m,d,type='raw')
  data.frame(cl=colnames(p)[apply(p,1,which.max)],
             p=apply(p,1,max)
             )
}
nb.st <- function(train,test) {
  require(e1071,quietly=T)
  train <- train[,c('ID','Prod','Uprice','Insp')]
  train[which(train$Insp == 'unkn'),'Insp'] <- NA
  train$Insp <- factor(train$Insp,levels=c('ok','fraud'))
  model <- SelfTrain(Insp ~ .,train,
                     learner('naiveBayes',list()),'pred.nb')
  preds <- predict(model,test[,c('ID','Prod','Uprice','Insp')],
                   type='raw')
  return(list(rankOrder=order(preds[,'fraud'],decreasing=T),
              rankScore=preds[,'fraud'])
         )
}
ho.nb.st <- function(form, train, test, ...) {
  res <- nb.st(train,test)
  structure(evalOutlierRanking(test,res$rankOrder,...),
            itInfo=list(preds=res$rankScore,
                        trues=ifelse(test$Insp=='fraud',1,0)
                       )
           )
}


nb.st.res <- holdOut(learner('ho.nb.st',
                          pars=list(Threshold=0.1,
                                    statsProds=globalStats)),
                  dataset(Insp ~ .,sales),
                  hldSettings(3,0.3,1234,T),
                  itsInfo=TRUE
                  )
```

Self training evaluation
========================
```{r}
summary(nb.st.res)
```

-  Only slight improvement here over Naive Bayes.
-  Still not breaking out by model.

Plot evaluation
===============
```{r, echo=FALSE}
par(mfrow=c(1,2))
info <- attr(nb.st.res,'itsInfo')
PTs.nb.st <- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)),
                 c(1,3,2)
                )
PRcurve(PTs.nb[,,1],PTs.nb[,,2],
        main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1),
        avg='vertical')
PRcurve(PTs.orh[,,1],PTs.orh[,,2],
        add=T,lty=1,col='grey',
        avg='vertical')        
PRcurve(PTs.nb.st[,,1],PTs.nb.st[,,2],
        add=T,lty=2,
        avg='vertical')        
legend('topright',c('NaiveBayes','ORh','NaiveBayes-ST'),
       lty=c(1,1,2),col=c('black','grey','black'))
CRchart(PTs.nb[,,1],PTs.nb[,,2],
        main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1),
        avg='vertical')
CRchart(PTs.orh[,,1],PTs.orh[,,2],
        add=T,lty=1,col='grey',
        avg='vertical')        
CRchart(PTs.nb.st[,,1],PTs.nb.st[,,2],
        add=T,lty=2,
        avg='vertical')        
legend('bottomright',c('NaiveBayes','ORh','NaiveBayes-ST'),
       lty=c(1,1,2),col=c('black','grey','black'))
```

And self-training with AdaBoost
===============================
```{r, echo=FALSE}
pred.ada <- function(m,d) {
  p <- predict(m,d,type='probability')
  data.frame(cl=colnames(p)[apply(p,1,which.max)],
             p=apply(p,1,max)
             )
}
ab.st <- function(train,test) {
  require(RWeka,quietly=T)
  train <- train[,c('ID','Prod','Uprice','Insp')]
  train[which(train$Insp == 'unkn'),'Insp'] <- NA
  train$Insp <- factor(train$Insp,levels=c('ok','fraud'))
  model <- SelfTrain(Insp ~ .,train,
                     learner('AdaBoostM1',
                             list(control=Weka_control(I=100))),
                     'pred.ada')
  preds <- predict(model,test[,c('ID','Prod','Uprice','Insp')],
                   type='probability')
  return(list(rankOrder=order(preds[,'fraud'],decreasing=T),
              rankScore=preds[,'fraud'])
         )
}
ho.ab.st <- function(form, train, test, ...) {
  res <- ab.st(train,test)
  structure(evalOutlierRanking(test,res$rankOrder,...),
            itInfo=list(preds=res$rankScore,
                        trues=ifelse(test$Insp=='fraud',1,0)
                       )
           )
}
ab.st.res <- holdOut(learner('ho.ab.st',
                          pars=list(Threshold=0.1,
                                    statsProds=globalStats)),
                  dataset(Insp ~ .,sales),
                  hldSettings(3,0.3,1234,T),
                  itsInfo=TRUE
                  )
```

Self-training with AdaBoost results
===================================
```{r}
summary(ab.st.res)
```

Compare self-training with standard AdaBoost and ORh
======================================================
```{r, echo=FALSE}
par(mfrow=c(1,2))
info <- attr(ab.st.res,'itsInfo')
PTs.ab.st <- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)),
                 c(1,3,2)
                )
PRcurve(PTs.ab[,,1],PTs.ab[,,2],
        main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1),
        avg='vertical')
PRcurve(PTs.orh[,,1],PTs.orh[,,2],
        add=T,lty=1,col='grey',
        avg='vertical')        
PRcurve(PTs.ab.st[,,1],PTs.ab.st[,,2],
        add=T,lty=2,
        avg='vertical')        
legend('topright',c('AdaBoostM1','ORh','AdaBoostM1-ST'),
       lty=c(1,1,2),col=c('black','grey','black'))
CRchart(PTs.ab[,,1],PTs.ab[,,2],
        main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1),
        avg='vertical')
CRchart(PTs.orh[,,1],PTs.orh[,,2],
        add=T,lty=1,col='grey',
        avg='vertical')        
CRchart(PTs.ab.st[,,1],PTs.ab.st[,,2],
        add=T,lty=2,
        avg='vertical')        
legend('bottomright',c('AdaBoostM1','ORh','AdaBoostM1-ST'),
       lty=c(1,1,2),col=c('black','grey','black'))
```

AdaBoost self-training results
===============================

-  Some improvement using self-training.
-  Not very impressive.
-  But need to compare cost of inspections against probability of a transaction being fraudulent.

Summary for Detecting Fraudulent Transactions
=============================================
type: section

Methodology 
============

-  Outlier detection
-  Clustering methods
-  Semi-supervised methods
-  Semi-supervised methods with self-training
-  Inbalanced class distributions and methods for dealing with them
-  Naive Bayes
-  AdaBoost

Evaluation methods
====================

-  Precision/Recall curves
-  Cumulative Recall curve
-  Hold-out experiments
