Predictor importance and feature selection
========================================================
author: IE 2064 Data Science
date: April 2015

Feature selection
===================

- One aspect of Big Data is Variety - The number of variables (features) that are available for each observation.
- Choosing which features to use in a predictive model is *feature selection*.
- *Supervised feature selection* is when the strength or relevance of the predictors is used to filter what should be the inputs in a model.
- Many predictive modeling methods have *intrinsic* feature selection where the model monitors the increase in performance as predictors are added.

Measuring predictor importance
==================
type:section

Data example
================

- Solubility of a large number of chemical compounds.
- Includes:
  -  Presence or absence of one of a large number of chemical *fingerprints*, representing a particular chemical structure.
  -  Sixteen count descriptors such as number of bonds or count of atoms of a specific element.
  -  Continuous descriptors (molecular weight or area)
- 1,267 compounds, split into a training and test set.


Predictor importance in numerical outcomes
===============

-  *Correlation statistic* - Expresses relationship between a candidate predictor and the outcome variable.
  -  *sample correlation* - For linear associations.
  -  *Spearman's correlation coefficient* - For nearly linear or curvilinear.
-  May not be useful for complex relationships.

```{r, echo=FALSE}
options(digits = 2)
library(AppliedPredictiveModeling)
library(caret)
library(lattice)
library(CORElearn)
library(corrplot)
library(pROC)
library(minerva)
```
```{r}
data(solubility)
cor(solTrainXtrans$NumCarbon, solTrainY)
```

Correlation plots
==========
```{r, echo=FALSE}
notFingerprints <- grep("FP", names(solTrainXtrans))

featurePlot(solTrainXtrans[, -notFingerprints],
            solTrainY,
            between = list(x = 1, y = 1),
            type = c("g", "p", "smooth"),
            labels = rep("", 2))
```

Example of more complex relationships
======
```{r, echo=FALSE}
trainData <- solTrainXtrans
trainData$y <- solTrainY

## keep the continuous predictors and append the outcome to the data frame
SolContPred <- solTrainXtrans[, !grepl("FP", names(solTrainXtrans))]
numSolPred <- ncol(SolContPred)
SolContPred$Sol <- solTrainY

## Get the LOESS smoother and the summary measure
library(caret)
smoother <- filterVarImp(x = SolContPred[, -ncol(SolContPred)],
                         y = solTrainY,
                         nonpara = TRUE)
smoother$Predictor <- rownames(smoother)
names(smoother)[1] <- "Smoother"

## Calculate the correlation matrices and keep the columns with the correlations
## between the predictors and the outcome

correlations <- cor(SolContPred)[-(numSolPred+1),(numSolPred+1)]
rankCorrelations <- cor(SolContPred, method = "spearman")[-(numSolPred+1),(numSolPred+1)]
corrs <- data.frame(Predictor = names(SolContPred)[1:numSolPred],
                    Correlation = correlations,
                    RankCorrelation  = rankCorrelations)

## The maximal information coefficient (MIC) values can be obtained from the
### minerva package:

library(minerva)
MIC <- mine(x = SolContPred[, 1:numSolPred], y = solTrainY)$MIC
MIC <- data.frame(Predictor = rownames(MIC),
                  MIC = MIC[,1])

## The Relief values for regression can be computed using the CORElearn
## package:

library(CORElearn)
ReliefF <- attrEval(Sol ~ .,  data = SolContPred,
                    estimator = "RReliefFequalK")
ReliefF <- data.frame(Predictor = names(ReliefF),
                  Relief = ReliefF)

## Combine them all together for a plot
contDescrScores <- merge(smoother, corrs)
contDescrScores <- merge(contDescrScores, MIC)
contDescrScores <- merge(contDescrScores, ReliefF)

rownames(contDescrScores) <- contDescrScores$Predictor

# contDescrScores

contDescrSplomData <- contDescrScores
contDescrSplomData$Correlation <- abs(contDescrSplomData$Correlation)
contDescrSplomData$RankCorrelation <- abs(contDescrSplomData$RankCorrelation)
contDescrSplomData$Group <- "Other"
contDescrSplomData$Group[grepl("Surface", contDescrSplomData$Predictor)] <- "SA"

featurePlot(solTrainXtrans[, c("NumCarbon", "SurfaceArea2")],
            solTrainY,
            between = list(x = 1),
            type = c("g", "p", "smooth"),
            df = 3,
            aspect = 1,
            labels = c("", "Solubility"))
```

Look at correlations between variables
=====
```{r, echo=FALSE}
corrplot::corrplot(cor(solTrainXtrans[, -notFingerprints]),
                   order = "hclust",
                   tl.cex = .8)
```

What to do when the relationship is not simple?
=======

-  Locally weighted regression model (LOESS)
  - Based on a series of polynomial regressions in small neighborhoods in the data.

LOESS smoothing
=============

```{r, echo=FALSE}
xyplot(solTrainY ~ solTrainXtrans$NumCarbon,
         type = c("p", "smooth"),
         xlab = "# Carbons",
         ylab = "Solubility")
```
***
```{r, echo=FALSE}
xyplot(solTrainY ~ solTrainXtrans$SurfaceArea2,
         type = c("p", "smooth"),
         xlab = "# Carbons",
         ylab = "Solubility")
```

Comparing numeric predictors
============

-  Correlation
-  From LOESS, can calculate a pseudo-$R^2$
-  Maximal information coefficient
-  **Issue**: Each of these methods looks at one predictor at a time
  -  Does not recognize correlation among predictors.
  -  Does not identify groups of predictors that work together. (interaction between predictors is what is important)


Comparison of many predictor importance measures
========
```{r, echo=FALSE}
splom(~contDescrSplomData[,c(3, 4, 2, 5)],
      groups = contDescrSplomData$Group,
      varnames = c("Correlation", "Rank\nCorrelation", "LOESS", "MIC"))
```



Categorical predictors
=================

-  Standard method is the $t$ statistics.
  -  Compares the difference between the groups to a statistics measuring variance.

```{r, echo=FALSE}
SolCatPred <- solTrainXtrans[, grepl("FP", names(solTrainXtrans))]
SolCatPred$Sol <- solTrainY
numSolCatPred <- ncol(SolCatPred) - 1

tests <- apply(SolCatPred[, 1:numSolCatPred], 2,
                  function(x, y)
                    {
                    tStats <- t.test(y ~ x)[c("statistic", "p.value", "estimate")]
                    unlist(tStats)
                    },
               y = solTrainY)
## The results are a matrix with predictors in columns. We reverse this
tests <- as.data.frame(t(tests))
names(tests) <- c("t.Statistic", "t.test_p.value", "mean0", "mean1")
tests$difference <- tests$mean1 - tests$mean0
head(tests)
```


Examine a range of t-tests through volcano plot
=======
-  The *Volcano plot* looks compares p-value to difference in solubility means (i.e. significance vs. magnitude)
-  Take the $-log(p-value)$ vs magnitude of difference in multiples of $\sigma$
- Higher values of the y-axis are indicative of strong statistical significance (i.e., low p-value).

Tornado plot example
==========
```{r, echo=FALSE}
## Create a volcano plot

xyplot(-log10(t.test_p.value) ~ difference,
       data = tests,
       xlab = "Mean With Structure - Mean Without Structure",
       ylab = "-log(p-Value)",
       type = "p")
```

Comparing importance of a range of values
=============

-  To compare candidate predictors to each other, use Analysis of Variance.
-  ANOVA compares models based on their ability to explain the variation between samples.
-  e.g. what we did when looking at the amoeba counts in water samples.

Categorical outcomes
==========
type:subsection

Categorical outcomes
================

- Classification  methods.
- Similar to evaluating methods, we can use area under the ROC curve (AUC) to evaluate predictors being included by a model.
-  Also *t* statistics can be used to compare different predictors.

Compare classification metrics
======
```{r, echo=FALSE}
data(segmentationData)
segTrain <- subset(segmentationData, Case == "Train")
segTrain$Case <- segTrain$Cell <- NULL

segTest <- subset(segmentationData, Case != "Train")
segTest$Case <- segTest$Cell <- NULL

## Compute the areas under the ROC curve
aucVals <- filterVarImp(x = segTrain[, -1], y = segTrain$Class)
aucVals$Predictor <- rownames(aucVals)

## Cacluate the t-tests as before but with x and y switched
segTests <- apply(segTrain[, -1], 2,
                  function(x, y)
                    {
                    tStats <- t.test(x ~ y)[c("statistic", "p.value", "estimate")]
                    unlist(tStats)
                    },
               y = segTrain$Class)
segTests <- as.data.frame(t(segTests))
names(segTests) <- c("t.Statistic", "t.test_p.value", "mean0", "mean1")
segTests$Predictor <- rownames(segTests)

## Fit a random forest model and get the importance scores
library(randomForest)
set.seed(791)
rfImp <- randomForest(Class ~ ., data = segTrain,
                      ntree = 2000,
                      importance = TRUE)
rfValues <- data.frame(RF = importance(rfImp)[, "MeanDecreaseGini"],
                       Predictor = rownames(importance(rfImp)))

## Now compute the Relief scores
set.seed(791)

ReliefValues <- attrEval(Class ~ ., data = segTrain,
                         estimator="ReliefFequalK", ReliefIterations = 50)
ReliefValues <- data.frame(Relief = ReliefValues,
                           Predictor = names(ReliefValues))

## and the MIC statistics
set.seed(791)
segMIC <- mine(x = segTrain[, -1],
               ## Pass the outcome as 0/1
               y = ifelse(segTrain$Class == "PS", 1, 0))$MIC
segMIC <- data.frame(Predictor = rownames(segMIC),
                  MIC = segMIC[,1])


rankings <- merge(segMIC, ReliefValues)
rankings <- merge(rankings, rfValues)
rankings <- merge(rankings, segTests)
rankings <- merge(rankings, aucVals)
head(rankings)
```

And compare several different classification metrics
=========
```{r, echo=FALSE}
rankings$channel <- "Channel 1"
rankings$channel[grepl("Ch2$", rankings$Predictor)] <- "Channel 2"
rankings$channel[grepl("Ch3$", rankings$Predictor)] <- "Channel 3"
rankings$channel[grepl("Ch4$", rankings$Predictor)] <- "Channel 4"
rankings$t.Statistic <- abs(rankings$t.Statistic)

splom(~rankings[, c("PS", "t.Statistic", "RF", "MIC")],
      groups = rankings$channel,
      varnames = c("ROC\nAUC", "Abs\nt-Stat", "Random\nForest", "MIC"),
      auto.key = list(columns = 2))
```

Summary
=========

-  Often we have more attributes then can be feasibly explored through EDA.
-  Feature selection methods can rank and score attributes as a filter prior to using them in a model.

Feature selection
===================
type:section

Why is this needed?
=================

-  Because we collect and store more information, we have a large amount of data with many pieces of information.
-  We can create datasets with tens of thousands of predictors.
-  It is not practical to evaluate all predictors, we need a way to filter predictors to a smaller set before evaluation.

Two classes of methods
=====================

-  *Unsupervised* - Outcomes are ignored during the elimination of predictors.
  -  e.g. Principle component analysis, random forests
  -  Check for high correlations or those with sparse distributions.
-  *Supervised* - Predictors are specifically selected to reduce the complexity of the model.
  - Predictor importance methods.
  
  
Effects of using non-informative predictors
=====================

- Goal of feature selection is to only include predictors that may help in making predictions.
- Non-informative predictors will actually increase RMSE for most model types.
  - Some types of models perform feature selection implicitly
  - Tree and rule based models, MARS, Lasso
- Having more predictors to perform hypothesis testing increases the probability of identifying significance where there is none.

======
![http://xkcd.com/882](resources/significant.png)


Approaches for reducing the number of predictors
=====

- *Wrapper methods* - Evaluate multiple models that add or remove predictors to find the optimal combination to optimize model performance.
- *Filter methods* - evaluate the relevance of predictors, then only model using predictors that pass some criteria.

Wrapper methods
=============
type:subsection

-  Conduct a search of predictors in the context of the modeling method to identify the set of predictors with the best results.
-  Pro: Can look at sets of predictors as a whole.
-  Con: Many models have to be evaluated, so this is computationally intensive.

Some methods
==========

-  Forward selection
-  Backwards selection
-  Stepwise selection


Forward selection
=================

- Start with a simple model (only intercept)
- For each predictor not in the current model
  - Create a candidate by adding the predictor
  - Estimate statistical significance of new term through hypothesis testing
- If the smallest p-value is less than the inclusion threshold, add it to the current model.
- Done when no statistically significant predictors remain outside the model

Issues with forward search
=====================

1.  Search procedure is *greedy*, so does not reevaluate past solutions.
2.  Repeated hypothesis testing makes their value lower.
3.  Maximizing statistical significance is not the same thing as maximizing accuracy.

Better performance measures
===============

-  We can use measures that focus on predictive performance
-  RMSE, classification accuracy, AUC (ROC)
-  Add in criteria that penalize number of predictors
  - Akaike Information Criterion (AIC)
  $$AIC = n \quad log\left(\sum_{i=1}^n(y_i - \hat{y}_i)^2\right) + 2P$$
-  Look for correlations with outcome but week-between predictor correlations
  $$G = \frac{P R_y}{\sqrt{P + P(P-1)\bar{R}_x}}$$
  -  $R_x$ is correlation between predictor and outcome
  -  $R_y$ is average correlation with other predictors in model.
  
Backwards selection
===============

-  Start model with all *P* available predictors.
-  Remove iteratively those that do not significantly contribute to the model (using AIC or *p* values)

Stepwise selection
==================

-  Forward selection, but after a term is added to the model, evaluate the model for terms to remove.
-  May have different *p*-value criteria for adding and removal.

Simulated Annealing
===============

-  Search procedure based on models of metal cooling
-  Select an initial random subset of predictors.
  -  Randomly modify the predictor set
  -  Evaluate model (training, the calculate performance)
  -  If this is the best model, take this predictor set as the best
  -  If not, calculate a probability *p* to accept the current predictor set, then determine random outcome.
-  Determine which accepted predictor set had the best performance.

Genetic algorithms
===============

-  Optimization method based on combining existing solutions based on fitness, then generating new solutions using crosses between existing solutions and mutations.
-  Better current solutions will be more likely to be used as a basis for future solutions.
-  Mutations allow for searches of the overall solution space (you do not get trapped into a local solution)


Filter methods
============
type:subsection


-  Evaluate predictors prior to developing model.
-  Pro: Tends to be computationally efficient.
-  Con: Evaluates each predictor separately so does not account for correlations and interactions.

Why not predictor importance methods?
============

-  Many are based on hypothesis tests (e.g. *t*-tests)
-  If many hypothesis tests are made, *multiple comparisons* problem.
  - If you do 20 tests at *p*=0.05 . . .
  - Need to adjust *p*-values, usually using *p/M*
  - But this is too conservative.

