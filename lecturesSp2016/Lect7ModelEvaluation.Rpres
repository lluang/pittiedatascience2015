Model Evaluation Methods
==============
author: Louis Luangkesorn
date: February 2016

General principles for model evaluation
=============
type:section

Choosing between models
==================

-  General principles
-  Comparing predictors
-  Comparing classifications


General principles
=====================

1. Flexibility of models.
2. Models that lead to easy interpretation.
3. Simplicity

Flexibility
===========

-  Some models are very flexible
-  e.g. Boosting, Support Vector Machines
-  Likely to be very accurate emperically
-  Con: very difficult to interpret and gain understanding from the results

Interpretation
=============

-  Some model types lend themselves to easy interpretation
-  e.g. regression methods, naive Bayes, association rules
-  May be less accurate emperically, but leads to understanding about the system
-  Very useful if the goal is to inform a person making the decision as opposed to pure machine learning.

Simplicity
============

-  Use the simplest model that is a reasonable approximation of the complex model
   -  Note: a more complex model should be more accurate, but it may not be more accurate by much.
   -  Tradeoff simplicity for accuracy. If the simpler model is within some measure of error of the more complex one, use the simpler model since there is no significant improvement.


Considerations for predictive models
=============

-  Balance between accuracy and overfitting
  -  The goal of the model is to describe the system, not fit the available data.
-  Balance between identifying all members of a target class and overfitting to the data set you happen to have.

Methods for model evaluation
===============
type:section


Regression methods
===============

-  Mean Squared Error based methods
-  Distance methods

Classification methods
==================

-  Receiver operating characteristics
-  Sensitivity vs Specificity
-  Precision vs Recall
-  Lift

Overfitting
===========

- Our goal is to perform the best prediction on future samples, not the samples we already have.
- One danger is that our model is overly specific, it matches the data that we happen to have instead of being a good general model.
- Cross-validation
  1.  Obtain *k* equally sized and random subsets of the training data.
  2.  For each of these *k* subsets, build a model using the remaining *kâˆ’1* sets.
  3.  evaluate this model on the *k*th subset.

Summary
=======

When facing a predictive task, we have to make the following decisions:

- Select the alternative models to consider (the models can actually be alternative settings of the same algorithm) for the predictive task(s) we want to address.
- Select the evaluation metrics that will be used to compare the models.
- Choose the experimental methodology for obtaining reliable estimates
of these metrics.

Regression methods using caret
===============================
type: section

The `caret` package enables consistent implementation and evaluation of a range of machine learning models.
```{r}
library(AppliedPredictiveModeling)
library(caret)
```
Ordinary Linear Regression
================

- Regression requires a function and a data frame as input.
- Returns parameters and model evaluation statistics.

Linear model implementation
===========
- Specify data
-  Specify outcome variable
- Later we will split data for cross-evaluations

```{r}
data(FuelEconomy)
trainingData <- cars2010
lmFit1 <- lm(FE ~ . - FE, data = trainingData)
```

Linear regression output
========
```{r, echo=FALSE}
summary(lmFit1)
```

Look at R^2 and RMSE
=========

- To get RMSE, look at predictors

```{r}
lmFitPred1 <-predict(lmFit1, cars2010[,-4])
lmFitValues1 <- data.frame(obs=cars2010[,c("FE")], pred=lmFitPred1)
defaultSummary(lmFitValues1)
```

Model evaluation
================
- Model evaluation is best done through cross-validation
- Cross validation generates a model using training data
- Test using holdout data
- Multiple cross validation using different holdout data sets generate a mean and standard error for model predictive performance.
-  We can use statistical methods like Mean Squared Error based methods when the prediction is a value that can be compared to the actual value.
-  Similar to methods used in developing regression models (Least Squared Error)

Model criteria
==============

-  Mean absolute error (MAE) between predictions and real values of the target variables.
-  Mean squared error (MSE)
-  Normalized mean squared error (NMSE)

Scatter plot with prediction
======================

```{r}
carsplot <- cars2010
carsplot$Pred1 <- lmFitPred1
qplot(Pred1, FE, data=carsplot, main = "Linear Model", xlab = "Predictions", ylab = "True Values") + geom_smooth(method="lm")
```

Absolute error
====

-  Take the average of the absolute value of difference between prediction and actual value

```{r}
(mae.a1.lm <- mean(abs(carsplot$Pred1 - cars2010[,"FE"])))
```

Note that units are the same units as the measurement

Mean squared error
=================

- Square the errors to penalize distance between prediction and actual.
- Take average of squared errors.

MSE Calculations
=============
```{r}
(mse.a1.lm <- mean(mean(abs(carsplot$Pred1 - cars2010[,"FE"]))^2))
```

Normalized mean squared error
=============================

-  Divide by the square of the difference between observations and the mean.
-  NMSE controls for the variance in the population so it gives values that are comparable between sample populations.
-  Result is a ratio.

NMSE Calculations
==========

```{r}
(nmse.a1.lm <- mean(((carsplot$Pred1 - cars2010[,"FE"]))^2)/
   mean((mean(cars2010[,"FE"])-cars2010[,"FE"])^2))
```

Distance based metrics
=============

-  Create a metric that looks at the distance between the prediction and actual.
-  Note: this is context dependent!
-  Need to define a distance metric. (e.g. square root of some of squares)
-  Need to control for the variation that is in the data.
-  Normalization controls for the variation by dividing the distance by a measure of variation.

Normalized Distance
=======

-  Look at the distance between a prediction and actual by normalied against the InterQuartile Range (IQR).
-  NDTP(u) - Normalized distance to typical price
$$NDTP_p(u)=\frac{|u-\widetilde{U}_p|}{IQR_p}$$
-  Unit price of observation minus the median of the unit prices of the product divided by the IQR of the unit prices of the product.
  -  If IQR==0, set $IQR = \widetilde{U_p}$

```{r, echo=FALSE}
avgNDTP <- function(toInsp,train,stats) {
  if (missing(train) && missing(stats))
    stop('Provide either the training data or the product stats')
  if (missing(stats)) {
    notF <- which(train$Insp != 'fraud')
    stats <- tapply(train$Uprice[notF],
                    list(Prod=train$Prod[notF]),
                    function(x) {
                      bp <- boxplot.stats(x)$stats
                      c(median=bp[3],iqr=bp[4]-bp[2])
                    })
    stats <- matrix(unlist(stats),
                    length(stats),2,byrow=T,
                    dimnames=list(names(stats),c('median','iqr')))
    stats[which(stats[,'iqr']==0),'iqr'] <-
        stats[which(stats[,'iqr']==0),'median']
  }

  mdtp <- mean(abs(toInsp$Uprice-stats[toInsp$Prod,'median']) /
               stats[toInsp$Prod,'iqr'])
  return(mdtp)
}
```


Using cross validation
=========================

- Determine model type (e.g. Linear regression)
- Determine cross validation methods using `trainControl`
- Set random number seed
- Train model using random holdout sets based on cross validation method

Cross validation example
==========
```{r}
ctrl = trainControl(method="cv", number = 10)
set.seed(100)
lmFit1 <- train(x = cars2010[,-4], y=cars2010[,4],
                method="lm", trControl=ctrl)
lmFit1
```

Regression trees (with caret)
============================

Repeat model evaluation and comparison between regression trees and linear regression
===========================================


Cross validation 
==================

Subset selection methods
==================================
type: subsection

Subset selection
===================

-  To evaluate a regression model, we can look at the coefficient of variation of the error.
-  $CV = \frac{\sigma}{\mu}$
-  But, in general, more terms lead to lower $\sigma$.
-  Need to balance lower errors against model complexity

![Residual SS for all possible subsets](resources/subsetselectionrss.png)

Results of cross-validation
================================

-  For each candidate model (subset size), cross validation will generate a series of estimates of error.
-  This gives the *estimated prediction error* and standard error bands
-  Identify the best model.
-  Take the sum of the mean error plus the standard error (standard deviation of error).
-  Find the model with the smallest subset size that is less than this sum.

Estimated prediction error curves
=============================

![Estimated Prediction Error Curves](resources/estimatedpredictionerror.png)


Classification methods
==================
type: section

**REMOVE SECTION ON CLASSIFICATION METHODS**
**LECTURE 17 USES CARET FOR CLASSIFICATION METHODS**

Methods for evaluating classifications
========

-  Confusion matrix
-  Precision vs Recall
-  Sensitivity vs Specificity
-  Receiver operating characteristics
-  Sensitivity vs Specificity
-  Lift


Confusion matrix
=====================

-  *Precision* - Proportion of event signals created by the model that are correct.
-  *Recall* - Proportion of events that are signaled by the models.
-  Confusion matrix

![Confusion matrix](resources/confusionmatrix.png)

Precision and recall
====================

- Precision
$$Prec=\frac{n_{s,s} + n_{b,b}}{N_{.,s}+N_{.,b}}$$
- Recall
$$Rec=\frac{n_{s,s} + n_{b,b}}{N_{s,.}+N_{b,.}}$$
- Combine these to form the *F-measure*
$$F=\frac{\beta^2+1 * Prec * Rec}{\beta^2 * Prec + Rec}$$
- Where $0\leq \beta \leq 1$
- *F-measure*  uses $\beta$ to specify the relative importance of recall to precision.

Precision - recall curve example
=====================
```{r}
library(ROCR)
data(ROCR.simple)
pred <- prediction(ROCR.simple$predictions, ROCR.simple$labels)
perf <- performance(pred, "prec", "rec")
plot(perf)
```



Sensitivity and specificity
=======================

-  For a two-class problem, a prediction can be evaluated in two ways

1.  The rate that an event is predicted correctly *sensitivity* - or *True positive*
2.  The rate that non-events are predicted correctly *specificity* - or *True negative*
  -  1 - *specificity* =  *False positive*

Sensitivity and specificity
===================

-  Sensitivity

$$\frac{\text{\# Samples with the event AND predicted to have the event}}{\text{\# samples having the event}}$$

-  Specificity

$$\frac{\text{\# Samples without the event AND predicted as nonevents}}{\text{\# samples without the event}}$$

- *J index* combines these
$$J = Sensitivity + Specificity -1$$

Positive and Negative Predictive Value
=====================

-  PPV - If the prediction method predicts positive, what is the probability of the sample being positive?
-  NPV - If the prediction method predicts negative, what is the probability of the sample being negative?


Sensitivity and specificity
==================

Predicted | Observed |_
----------|----------|------------
          | **Event**    | **Non-event**
Event     | TP       |  FP
Non-event | FN       |  TN


```{r}
sensspec <- performance(pred, "sens", "spec")
plot(sensspec)
```

Receiver Operating Characteristic (ROC)
==================

-  Taken from the radio electronics industry.
-  Can the instrument detect a signal from noise.
-  Combines Sensitivity and Specificity
-  Plot *1 - Specificity* against *Sensitivity* (FPR vs TPR)
-  AUC - Area Under ROC curve

ROC curve
========

-  Evaluate a model against a continuum of thresholds.
-  Thresholds are scores used to separate the range of possible observations into two potential classes.
-  For each threshold, determine the resulting Specificity and Sensitivity.
-  Plot on ROC chart.
-  A perfect model has 100% Sensitivity and specificity.
-  In practice, higher sensitivity leads to lower specificity (being able to identify all representatives of a class leads to also having false positives)


True positive rate vs false positive rate
=====

```{r}
pred <- prediction(ROCR.simple$predictions, ROCR.simple$labels)
perf <- performance(pred, "tpr", "fpr")
plot(perf, main="ROC Curve")
```


Lift charts
=============

-  Evaluate a model against a complete random selection of samples.

1.  Predict a set of samples that were not used in building the model but have known outcomes.
2.  Determine the *baseline* event rate, i.e. the percentage of true events in the data set.
3.  Using the model, order the data by classification probability (i.e. the samples most likely to be events first)
4.  For each class probability value, calculate the actual percentage of true eents in all samples below the probability value.
5. Divide the percent of true events for each probability threshold by the baseline event rate.  Fraction is the lift (>1.0)
- Cumulative recall charts also

Lift chart example
============

```{r}
par(mfrow=c(1,2))
pred <- prediction(ROCR.simple$predictions, ROCR.simple$labels)
perf <- performance(pred, "lift", "rpp")
plot(perf, main="Lift Chart")
cp <- performance(pred, "rec", "rpp")
plot(cp, main="Cumulative recall chart")
par(mfrow=c(1,1))
```


