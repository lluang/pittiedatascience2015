Clustering & SMOTE
========================================================
author: IE2064 Data Science
date: March 2016

Overview
=======

-  Clustering
-  Classification
-  Synthetic Minority Oversampling Technique (SMOTE)
-  Model evaluation

Clustering
========================================================

-  Clustering is one the four core groups of data mining techniques.
-  Group observations based on how similar they are.
-  Break data into meaningful groups then contract the clusters against each other.
-  Group observations to make datasets easier to manage.

Application
===========

- Unsupervised clustering was used to look at fraud cases.
- The transactions that were different than the majority were viewed as potential fraud.
- When there is no training data, clustering will identify cases that are different from each other, then you can determine what made them different.
- Contrast: classification methods are used when you know what makes them different, and you are trying to determine which samples should be grouped together.

Clustering methods
====================

- *k*-means
- Hierarchical clustering
- Modified box plot rule
- Local outlier factors (LOF)
- Clustering based Outlier Rankings (ORh)
- Principal Component Analysis

Classification methods
=============================

-  Logistic regression
-  Naive Bayes
-  *Adaboost* classifiers
-  Random Forests
-  *k*-Nearest Neighbors

Evaluation
===========

-  Cross-validation
-  Precision/Recall and cumulative recall curves.
-  Hold-out experiments

Example
====
type:section


Romano-British pottery data
===========================

- Data is chemical composition of 48 specimens of Romano-British pottery
-  $Al_2O_3, FE_2O_3, MgO, CaO, Na_2O, K_2O, TiO_2, MnO, BaO$
-  Five sites (kilns)
-  Goal:  Can we tell which kiln each specimen came from?
-  Can we divide up the specimens into groups, and do the groups cross kilns?

Pottery data
============

- Included in package that goes along with the book *A Handbook of Statistical Analysis Using R, 2nd edition*

```{r}
library(HSAUR2)
library("mclust")
library("mvtnorm")
library("lattice")
data(pottery)
summary(pottery)
```

Analysis steps
================

-  Create a distance matrix based on euclidean distance ($\sqrt{\sum squares}$)
-  It looks like there are distinct groupings (sets of pots that are close to each other)

```{r}
pottery_dist <- dist(pottery[, colnames(pottery) != "kiln"])
levelplot(as.matrix(pottery_dist), xlab = "Pot Number", 
          ylab = "Pot Number")
```

Try Hierarchical clustering
=======
```{r}
pottery_single <- hclust(pottery_dist, method = "single")
pottery_complete <- hclust(pottery_dist, method = "complete")
pottery_average <- hclust(pottery_dist, method = "average")
```

-  Branches that fuse later are more similar to each other.
-  Make a horizontal cut across the graph. This defines the clusters.

Hierarchical results
=======
-  Look at each cluster, how similar are they to each other? 

```{r, echo=FALSE}
layout(matrix(1:3, ncol = 3))
plot(pottery_single, main = "Single Linkage", 
     sub = "", xlab = "")
plot(pottery_complete, main = "Complete Linkage", 
     sub = "", xlab = "")
plot(pottery_average, main = "Average Linkage", 
     sub = "", xlab = "")
```





K-means clustering
==================
type: section

K-means
==========

-  *k*-means techniques seeks to partition a set of data into a specific number of groups by minimizing some numerical criteria.
-  There are a combinatorial number of ways to do this, so cannot use exhaustive search to find the best way.

Algorithm
============

1.  Find an initial set of *k* clusters (e.g. randomly)
2.  Calculate mean value for each cluster.
3.  Measure distance between each observation and each of the *k* vectors of mean values (for each cluster)
4.  Assign each observation to the closest of the *k* mean values.
5.  Repeat steps 2-4 until the clusters are stable.

Distance metrics
================

-  Euclidean distance
$$d(a,b) = \sqrt{\sum_{i \in N} (a_i - b_i)^2}$$
-  Manhattan distance
$$d(a,b) =\sum_{i \in N} (|a_i - b_i|)$$

Output
=========

-  Set of *k* centers of clusters.
-  Assign each observation to nearest mean.
-  Evaluation - Within cluster sum of squares.

Tuning k-means
==============

-  Choice of distance metric.
-  Number of clusters. (can iterate)
-  Use HClust centers as a starting point instead of random clusters.
-  Weighted *k*-means 

Hierarchical clustering
=======================
type: section

- Classification comes from a series of clusters.
- Agglomerative hierarchical clustering uses successive fusions (merges) of individuals into groups.
- Clustering goes from all single member groups to a single large group.
- Evaluation determines at what point you should have stopped (similar to recursive partitioning)

Types of linkages (agglomeration method)
==========

1.  Complete - Maximal intercluster dissimularity. Compute all pairs of dissimilarities between cluster A and cluster B, record the largest.
2.  Single - Minimal intercluster dissimularity. Compute all pairs of dissimilarities, record the smallest.
3.  Average - Mean intercluster dissimularity. Compute all pairs of dissimilarities, record the average.
4.  Centroid - Record the dissimilarity between the centroid of A and the centroid of B. (for each cluster, take the average of each dimension of measurement)

Algorithm
==========

1.  Determine what are the closest two cluster. Merge them together.
2.  If the number of clusters remaining equals 1, stop, otherwise repeat from step 1.
3. Decide where you should have stopped.


Tuning Hierarchical clustering
============================

- Choice of distance metric
- Agglomeration method (linkage)
- How many clusters to have



Detecting fraudulent transactions
=================================
type: section

Detecting fraudulent transactions
=================================

-  Goal:  Detect unusual events.
-  Example:  Salesperson transaction reports
-  What makes this hard?
-  What type of problem is this?

Data mining tasks
=================

-  Outlier detection
-  Classification
-  Semi-supervised learning

Outlier detection
=================
type: sub-section

Fraud detection
===============

-  Frauds are usually associated with unusual observations that are deviations from the norm.
-  *outlier* - An observation which deviates so much from other observations as to arouse suspicions that it was generated by a different mechanism.

Data set
========

-  Transactions reported by sales people.
-  Salespeople have flexibility in setting selling price.
-  Report transactions on a regular basis.
-  Question: How to verify transactions and efficiently use fraud investigation resources?

Data elements
=============

- ID – ID of the salesman.
- Prod – ID of the sold product.
- Quant – the number of reported sold units.
- Val – the reported total monetary value of the sale.
- Insp – `ok` if the transaction was
inspected and considered valid by the company, `fraud` if the transaction
was found to be fraudulent, and `unkn` if the transaction was not inspected
at all by the company.

Look at the data
================

```{r salesdata, echo=FALSE}
library(DMwR)
data(sales)
head(sales)
```

Summary of data
===============
```{r salessummary, echo=FALSE}
summary(sales)
```

Look at unique products and salespeople
=======================================
```{r}
print(nlevels(sales$ID))
print(nlevels(sales$Prod))
```

Look at the NAs, do they occur together?
========================================

```{r}
length(which(is.na(sales$Quant) & is.na(sales$Val)))
```

Now, look at the proportion of frauds of those inspected
=========================================================

```{r}
table(sales$Insp)/nrow(sales) * 100
```

Now look at the transactions
============================
```{r, echo=FALSE}
totS <- table(sales$ID)
totP <- table(sales$Prod)
barplot(totS,main='Transactions per salespeople',names.arg='',xlab='Salespeople',ylab='Amount')
```
***
```{r, echo=FALSE}
barplot(totP,main='Transactions per product',names.arg='',xlab='Products',ylab='Amount')
```

Look at the data elements that have lots of variability
==========================================================

-  Create a new measure of price per unit
```{r}
sales$Uprice <- sales$Val/sales$Quant
summary(sales$Uprice)
```

There are lots of products, look at the least and most expensive
================================================================

```{r}
upp <- aggregate(sales$Uprice,list(sales$Prod),median,na.rm=T)
topP <- sapply(c(T,F),function(o) 
               upp[order(upp[,2],decreasing=o)[1:5],1])
colnames(topP) <- c('Expensive','Cheap')
topP
```

Now, look at the distribution of unit price for these items
============================================================
```{r}
tops <- sales[sales$Prod %in% topP[1,],c('Prod','Uprice')]
tops$Prod <- factor(tops$Prod)
boxplot(Uprice ~ Prod,data=tops,ylab='Unit price',log="y")
```

Change focus to salespeople in terms of value 
==============================================

```{r}
vs <- aggregate(sales$Val,list(sales$ID),sum,na.rm=T)
scoresSs <- sapply(c(T,F),function(o) 
                   vs[order(vs$x,decreasing=o)[1:5],1])
colnames(scoresSs) <- c('Most','Least')
scoresSs
print(sum(vs[order(vs$x,decreasing=T)[1:100],2])/sum(sales$Val,na.rm=T)*100)
print(sum(vs[order(vs$x,decreasing=F)[1:2000],2])/sum(sales$Val,na.rm=T)*100)
```

Top products in terms of volume
==============================================

```{r}
qs <- aggregate(sales$Quant,list(sales$Prod),sum,na.rm=T)
scoresPs <- sapply(c(T,F),function(o) 
                   qs[order(qs$x,decreasing=o)[1:5],1])
colnames(scoresPs) <- c('Most','Least')
scoresPs
sum(as.double(qs[order(qs$x,decreasing=T)[1:100],2]))/
  sum(as.double(sales$Quant),na.rm=T)*100
sum(as.double(qs[order(qs$x,decreasing=F)[1:4000],2]))/
  sum(as.double(sales$Quant),na.rm=T)*100
```

First thoughts
==============

-  The top 100 products represent 75% of volume.
-  Bottom 1000 products represent less than 10%.
-  What should the distribution of price per unit of a given product be?
-  Remember what the process of setting the price is.

Outliers
=========

Using the box and whisker plot
-  *Interquartile range* (IQR) - the distance between the 25th (Q1) and 75th (Q3) quantiles.
-  Top whisker = $Q3 + 1.5 IQR$
-  Bottom whisker = $Q1 - 1.5 IQR$
-  Outliers are points outside the whiskers.

Look for outliers
=================

```{r}
out <- tapply(sales$Uprice,list(Prod=sales$Prod), 
              function(x) length(boxplot.stats(x)$out))
out[order(out,decreasing=T)[1:10]]
```
```{r}
sum(out)/nrow(sales)*100
```

Data munging
============

-  What to do with unknown values?

1. remove the cases, 
2. fill in the unknowns using some strategy, or 
3. use tools that handle these types of values.

-  We are looking for fraud, so we don't want to use tools that manipulate the data.

```{r, echo=FALSE}
totS <- table(sales$ID)
totP <- table(sales$Prod)
```

First thought, remove unknowns
===============================

-  Check to see if this causes problems by certain salespeople being overly represented.

```{r}
nas <- sales[which(is.na(sales$Quant) & is.na(sales$Val)),c('ID','Prod')]
propS <- 100*table(nas$ID)/totS
propS[order(propS,decreasing=T)[1:10]]
```

-  Not too bad.

Check products for NA
=====================
```{r}
propP <- 100*table(nas$Prod)/totP
propP[order(propP,decreasing=T)[1:10]]
```

-  Some of these are real high.
-  We could check to see if in general these had the same distributions as some other products. (yes) 
-  If so, we could combine products.

Try removing anything with NA in both value and quantity
========================================================

```{r}
sales1 <- sales[-which(is.na(sales$Quant) & is.na(sales$Val)),]
nnasQp <- tapply(sales1$Quant,list(sales1$Prod),
                 function(x) sum(is.na(x)))
propNAsQp <- nnasQp/table(sales1$Prod)
propNAsQp[order(propNAsQp,decreasing=T)[1:10]]
```

Note the 1.0000. Let's remove these


Remove the products with no transaction information
===================================================
```{r}
sales2 <- sales1[!sales1$Prod %in% c('p2442','p2443'),]
nlevels(sales2$Prod)
sales2$Prod <- factor(sales2$Prod)
nlevels(sales2$Prod)
```

Check salespeople for no quantity information
=============================================

```{r}
nnasQs <- tapply(sales2$Quant, list(sales2$ID), function(x) sum(is.na(x)))
propNAsQs <- nnasQs/table(sales2$ID)
propNAsQs[order(propNAsQs, decreasing = T)[1:10]]
```
- Should we remove the sales people without quantity information?

Look for transactions without quantity information 
===================================================
```{r}
nnasVp <- tapply(sales$Val,list(sales$Prod),
                 function(x) sum(is.na(x)))
propNAsVp <- nnasVp/table(sales$Prod)
propNAsVp[order(propNAsVp,decreasing=T)[1:10]]
```

Assume that there is a 'typical unit price'
===========================================

```{r}
tPrice <- tapply(sales2[sales2$Insp != 'fraud','Uprice'],list(sales2[sales2$Insp != 'fraud','Prod']),median,na.rm=T)
```

Fill in missing values
======================

-  Since there are no longer any transactions with both `Quant` and `Val` both missing, use them to fill in the missing values.
-  Recalculate unit prices
```{r}
noQuant <- which(is.na(sales2$Quant))
sales2[noQuant,'Quant'] <- ceiling(sales2[noQuant,'Val'] /
                       tPrice[sales2[noQuant,'Prod']])
noVal <- which(is.na(sales2$Val))
sales2[noVal,'Val'] <- sales2[noVal,'Quant'] *
                   tPrice[sales2[noVal,'Prod']]
sales2$Uprice <- sales2$Val/sales2$Quant
```

Data mining
=================
type: section

What type of problem is this?
=============================

-  Goal of data mining is to direct resources for investigating fraud.
-  The data includes a field labeled *Insp*, which is filled in for a small portion of the observations.

Unsupervised methods
=====================

- For most observations we have descriptors, but no inspection information.
- *Supervised  learning* assumes that there is a *teacher* that can identify some examples of concepts.
- *Unsupervised learning* has do training of the concept being searched for.

Descriptive data mining
========================

-  Clustering finds *natural* groupings of a set of observations by forming clusters of cases similar to each other.
-  How to define *similar*?
-  E.g. outlier detection.

Some unsupervised learning techniques
======================================

-  Modified box plot rule
-  Local Outlier factors
-  Clustering-based Outlier Rankings ($OR_h$)

Supervised learning techniques
==============================

-  A teacher *labels* observations (e.g. valid or fraud)
-  Goal: obtain a model that relates the target variable to independent variables (predictors, attributes)
-  Find model parameters that optimize some criteria.
-  See if the model can correctly label other data.
-  Focus on accurately predicting the less frequent class. (e.g. frauds)

Semi-supervised techniques
==========================

-  We have a small amount of labeled data, with a large proportion of unlabeled data.
-  Two types
  1.  Improve the performance of supervised classification methods by using information provided by unlabeled cases.
  2.  Use the labeled data to bias the groups being formed by the clustering methods.
-  Examples
  1.  Self-training - Use labeled data to develop the model then label unlabeled data.
  2.  Transductive support vector machines (TSVM) - obtain labels for a set of unlabeled data so that the boundary achieves the maximum margin on both labeled and unlabeled data.
  
Evaluation criteria
=====================

-  Needs to cover both labeled and unlabeled data.
-  Precision and recall
- Lift charts
- Normalized Distance to Typical Price

Precision/Recall curves
=======================

-  Visual representations of the performance of the model in terms of precision and recall.
-  Can be used to determine cut off points of models.
-  For a given performance measure, choose the top *k* values.

Precision/Recall curve example
==============================

-  Take a data set, make predictions.
-  Compare to labels.

```{r}
library(ROCR)
data(ROCR.simple)
pred <- prediction( ROCR.simple$predictions, ROCR.simple$labels )
perf <- performance(pred,'prec','rec')
plot(perf)
```

Making a smooth version
=======================

```{r, echo=FALSE}
plot(perf)
```
***
```{r, echo=FALSE}
PRcurve <- function(preds,trues,...) {
  require(ROCR,quietly=T)
  pd <- prediction(preds,trues)
  pf <- performance(pd,'prec','rec')
  pf@y.values <- lapply(pf@y.values,function(x) rev(cummax(rev(x))))
  plot(pf,...)
}
PRcurve(ROCR.simple$predictions, ROCR.simple$labels )
```


Testing the model
=================

-  Generate an outlier score for each observation (i.e. is the observation an outlier)
-  Order the observations by score.
-  Select a threshold (implies a limit on the effort to be expended using inspections).
-  Example:  top scores are
*fraud, unknown, fraud, f raud, unknown, ok, ok*

Confusion matrix
================

      |       |   ok  | fraud |
------|-------|-------|-------|---
True  | ok    |   3   |  1    | 4
Values| fraud |  2    |  1    | 3
      |       |  5    |  2    | 7
      
Lift charts
============

-  Focus on the recall.
-  Value of the rate of positive predictions (RPP)
-  Portion of positive class predictions/test cases

Lift chart plot
===============
```{r}
pred <- prediction( ROCR.simple$predictions, ROCR.simple$labels )
perf <- performance(pred,'lift','rpp')
plot(perf,main='Lift Chart')
```

Cumulative recall chart
=======================

```{r, echo=FALSE}
CRchart <- function(preds,trues,...) {
  require(ROCR,quietly=T)
  pd <- prediction(preds,trues)
  pf <- performance(pd,'rec','rpp')
  plot(pf,...)
}  
CRchart(ROCR.simple$predictions, ROCR.simple$labels, 
        main='Cumulative Recall Chart')
```

Normalized distance to Typical Price
=====================================

-  Now look at metrics for unlabeled cases.
-  Need measures that have been normalized to the scale or deviation of the data
-  NDTP(u) - Normalized distance to typical price
$$NDTP_p(u)=\frac{|u-\tilde{U}_p|}{IQR_p}$$

```{r, echo=FALSE}
avgNDTP <- function(toInsp,train,stats) {
  if (missing(train) && missing(stats)) 
    stop('Provide either the training data or the product stats')
  if (missing(stats)) {
    notF <- which(train$Insp != 'fraud')
    stats <- tapply(train$Uprice[notF],
                    list(Prod=train$Prod[notF]),
                    function(x) {
                      bp <- boxplot.stats(x)$stats
                      c(median=bp[3],iqr=bp[4]-bp[2])
                    })
    stats <- matrix(unlist(stats),
                    length(stats),2,byrow=T,
                    dimnames=list(names(stats),c('median','iqr')))
    stats[which(stats[,'iqr']==0),'iqr'] <- 
        stats[which(stats[,'iqr']==0),'median']
  }

  mdtp <- mean(abs(toInsp$Uprice-stats[toInsp$Prod,'median']) /
               stats[toInsp$Prod,'iqr'])
  return(mdtp)
}
```

Experimental Methodology
========================

-  Partition into test and training data.
- Stratified sampling of labeled and unlabeled data.
```{r}
evalOutlierRanking <- function(testSet,rankOrder,Threshold,statsProds) {
  ordTS <- testSet[rankOrder,]
  N <- nrow(testSet)
  nF <- if (Threshold < 1) as.integer(Threshold*N) else Threshold
  cm <- table(c(rep('fraud',nF),rep('ok',N-nF)),ordTS$Insp)
  prec <- cm['fraud','fraud']/sum(cm['fraud',])
  rec <- cm['fraud','fraud']/sum(cm[,'fraud'])
  AVGndtp <- avgNDTP(ordTS[nF,],stats=statsProds)
  return(c(Precision=prec,Recall=rec,avgNDTP=AVGndtp))
}
```

Obtaining Outlier Rankings
==========================
type: section

Unsupervised approaches
========================
type: sub-section

-  Modified box plot rule
-  Local Outlier factors
-  Clustering-based Outlier Rankings ($OR_h$)

Modified box plot rule
=======================

-  Use the NDTP as a variation of the box plot rule.
-  NDTP is unitless, so can be used to compare differnt producs.
```{r}
BPrule <- function(train,test) {
  notF <- which(train$Insp != 'fraud')
  ms <- tapply(train$Uprice[notF],list(Prod=train$Prod[notF]),
               function(x) {
                 bp <- boxplot.stats(x)$stats
                 c(median=bp[3],iqr=bp[4]-bp[2])
               })
  ms <- matrix(unlist(ms),length(ms),2,byrow=T,
               dimnames=list(names(ms),c('median','iqr')))
  ms[which(ms[,'iqr']==0),'iqr'] <- ms[which(ms[,'iqr']==0),'median']
  ORscore <- abs(test$Uprice-ms[test$Prod,'median']) /
             ms[test$Prod,'iqr']
  return(list(rankOrder=order(ORscore,decreasing=T),
              rankScore=ORscore))
}
```

Parameters of box plot rule
===========================

-  Training and test data sets.
-  Calculate median and IQR for each product.
-  Determine Outlier Score.
-  Rank order products by score.

Testing method
==============

-  Use hold-out experimental methodology.
-  Calculate median and IQR for each product.
- Calculate average NDTP score.

```{r, echo=FALSE}
notF <- which(sales$Insp != 'fraud')
globalStats <- tapply(sales$Uprice[notF],
                      list(Prod=sales$Prod[notF]),
                      function(x) {
                        bp <- boxplot.stats(x)$stats
                        c(median=bp[3],iqr=bp[4]-bp[2])
                      })
globalStats <- matrix(unlist(globalStats),
                length(globalStats),2,byrow=T,
                dimnames=list(names(globalStats),c('median','iqr')))
globalStats[which(globalStats[,'iqr']==0),'iqr'] <- 
   globalStats[which(globalStats[,'iqr']==0),'median']
```

Hold out - Box plot rule
========================

-  Hold out with the Box Plot Rule
-  Use 70%/30% division of the datset
-  Inspection limit effort (Threshold) of 10%

```
ho.BPrule <- function(form, train, test, ...) {
  res <- BPrule(train,test)
  structure(evalOutlierRanking(test,res$rankOrder,...),
            itInfo=list(preds=res$rankScore,
                        trues=ifelse(test$Insp=='fraud',1,0)
                       )
           )
}
bp.res <- holdOut(learner('ho.BPrule',
                          pars=list(Threshold=0.1,
                                    statsProds=globalStats)),
                  dataset(Insp ~ .,sales),
                  hldSettings(3,0.3,1234,T),
                  itsInfo=TRUE
                  )
save(bp.res, file="bp.res.Rdata")
```

Results of ho.bprule
====================
```{r, echo=FALSE}
load("BPresults.Rdata")
summary(bp.res)
#  Note Precision and recall look low
#  Normalized Distance to Typical Price high.
```

Precision/Recall curves
=======================

-  How much effort is needed to get a good amount of recall?
```{r, echo=FALSE}
par(mfrow=c(1,2))
info <- attr(bp.res,'itsInfo')
PTs.bp <- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)),
                c(1,3,2)
               )
PRcurve(PTs.bp[,,1],PTs.bp[,,2],
        main='PR curve',avg='vertical')
CRchart(PTs.bp[,,1],PTs.bp[,,2],
        main='Cumulative Recall curve',avg='vertical')
```

Local Outlier Factors (LOF)
===========================

-  Generate an outlyingess score for each case by estimating its degree of isolation with its local neighborhood.
-  *core distance* - distance between a point *p* and its *k*th nearest neighbor.
-  *reachability distance* - The maximum of the core distnce of $p_1$ and the distance between $p_1$ and $p_2$
-  *local reachability distance* - the average reachability distance between a point $p_1$ and its *k* nearest neighbors.

Implementation
===============

```
ho.LOF <- function(form, train, test, k, ...) {
  require(dprep,quietly=T)
  ntr <- nrow(train)
  all <- rbind(train,test)
  N <- nrow(all)
  ups <- split(all$Uprice,all$Prod)
  r <- list(length=ups)
  for(u in seq(along=ups)) 
    r[[u]] <- if (NROW(ups[[u]]) > 3) 
                 lofactor(ups[[u]],min(k,NROW(ups[[u]]) %/% 2)) 
              else if (NROW(ups[[u]])) rep(0,NROW(ups[[u]])) 
              else NULL
  all$lof <- vector(length=N)
  split(all$lof,all$Prod) <- r
  all$lof[which(!(is.infinite(all$lof) | is.nan(all$lof)))] <- 
    SoftMax(all$lof[which(!(is.infinite(all$lof) | is.nan(all$lof)))])
  structure(evalOutlierRanking(test,order(all[(ntr+1):N,'lof'],
                                           decreasing=T),...),
            itInfo=list(preds=all[(ntr+1):N,'lof'],
                        trues=ifelse(test$Insp=='fraud',1,0))
           )
}


lof.res <- holdOut(learner('ho.LOF',
                          pars=list(k=7,Threshold=0.1,
                                    statsProds=globalStats)),
                  dataset(Insp ~ .,sales),
                  hldSettings(3,0.3,1234,T),
                  itsInfo=TRUE
                  )
save(lof.res, file="lof.res.Rdata")
```

LOF summary
===========
```{r, echo=FALSE}
load("LOFresults.Rdata")
summary(lof.res)
```

Compare LOF and BPRule
======================

```{r, echo=FALSE}
par(mfrow=c(1,2))
info <- attr(lof.res,'itsInfo')
PTs.lof <- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)),
                 c(1,3,2)
                )
PRcurve(PTs.bp[,,1],PTs.bp[,,2],
        main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1),
        avg='vertical')
PRcurve(PTs.lof[,,1],PTs.lof[,,2],
        add=T,lty=2,
        avg='vertical')
legend('topright',c('BPrule','LOF'),lty=c(1,2))
CRchart(PTs.bp[,,1],PTs.bp[,,2],
        main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1),
        avg='vertical')
CRchart(PTs.lof[,,1],PTs.lof[,,2],
        add=T,lty=2,
        avg='vertical')
legend('bottomright',c('BPrule','LOF'),lty=c(1,2))
```

Clustering-Based Outlier Rankings (ORh)
=======================================

-  $OR_h$ method uses a heirarchal agglomerative clustering algorithm to obtain a *dendrogram*.
-  *dendrogram* visual representation of clustering methods.
-  $OR_h$ takes observations, then makes decisions to merge observations/groups that are similar to each other, two groups at a time.
-  Outliers will tend to offer resistance to mergeing, and when they do, will cause the new group to be changed in size.

ORh implementation
===================
```
###################################################
### Clustering-based outlier rankings (OR_h)
###################################################
ho.ORh <- function(form, train, test, ...) {
  require(dprep,quietly=T)
  ntr <- nrow(train)
  all <- rbind(train,test)
  N <- nrow(all)
  ups <- split(all$Uprice,all$Prod)
  r <- list(length=ups)
  for(u in seq(along=ups)) 
    r[[u]] <- if (NROW(ups[[u]]) > 3) 
                 outliers.ranking(ups[[u]])$prob.outliers 
              else if (NROW(ups[[u]])) rep(0,NROW(ups[[u]])) 
              else NULL
  all$lof <- vector(length=N)
  split(all$lof,all$Prod) <- r
  all$lof[which(!(is.infinite(all$lof) | is.nan(all$lof)))] <- 
    SoftMax(all$lof[which(!(is.infinite(all$lof) | is.nan(all$lof)))])
  structure(evalOutlierRanking(test,order(all[(ntr+1):N,'lof'],
                                           decreasing=T),...),
            itInfo=list(preds=all[(ntr+1):N,'lof'],
                        trues=ifelse(test$Insp=='fraud',1,0))
           )
}


orh.res <- holdOut(learner('ho.ORh',
                          pars=list(Threshold=0.1,
                                    statsProds=globalStats)),
                  dataset(Insp ~ .,sales),
                  hldSettings(3,0.3,1234,T),
                  itsInfo=TRUE
                  )
save(orh.res, file="orh.res.Rdata")
```

ORh results
===========
```{r,echo=FALSE}
load("ORHresults.Rdata")
summary(orh.res)
```

Comparison of BPrule, LOF, and ORh
==================================
```{r, echo=FALSE}
par(mfrow=c(1,2))
info <- attr(orh.res,'itsInfo')
PTs.orh <- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)),
                 c(1,3,2)
                )
PRcurve(PTs.bp[,,1],PTs.bp[,,2],
        main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1),
        avg='vertical')
PRcurve(PTs.lof[,,1],PTs.lof[,,2],
        add=T,lty=2,
        avg='vertical')
PRcurve(PTs.orh[,,1],PTs.orh[,,2],
        add=T,lty=1,col='grey',
        avg='vertical')        
legend('topright',c('BPrule','LOF','ORh'),
       lty=c(1,2,1),col=c('black','black','grey'))
CRchart(PTs.bp[,,1],PTs.bp[,,2],
        main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1),
        avg='vertical')
CRchart(PTs.lof[,,1],PTs.lof[,,2],
        add=T,lty=2,
        avg='vertical')
CRchart(PTs.orh[,,1],PTs.orh[,,2],
        add=T,lty=1,col='grey',
        avg='vertical')
legend('bottomright',c('BPrule','LOF','ORh'),
       lty=c(1,2,1),col=c('black','black','grey'))
```

Clustering - Supervised approaches
=======================================
type: section

-  Clustering based on using labeled data.

Class imbalance problem
========================

-  If a very large portion of the labeled data is in a single class, 
  -  One strategy would be to always predict the dominant class.
  -  Samples of the data will only have few minority cases to use in training.
-  Dealing with class imbalance
  1. Bias learning using metrics more sensitive to examples in the minority class.
  2. Sample using training data that manipulates the class distribution.
  
Techniques for working with class imbalance
===========================================

1.  Method that bias the learnning process by using evaluation metrics that are more sensitive to minority class examples.
2.  Sampling methods that manipulate the training data to change the class distribution.

Sampling methods that manipulate training data
==============================================

-  *Under-sampling*:  select a small part of the majority class examples to add to the minority class cases.
-  *Over-sampling*: use some process to replicate the minority class examples.

Synthetic Minority Oversampling Technique (SMOTE)
=====================================================

-  Artificially generates new examples of the minority class using the nearest neighbors of these cases. 
-  Undersample majority class so that the dataset is more balanced.

SMOTE with iris data
====================

```{r, echo=FALSE}
data(iris)
data <- iris[,c(1,2,5)]
data$Species <- factor(ifelse(data$Species == 'setosa','rare','common'))
table(data$Species)
newData <- SMOTE(Species ~ .,data,perc.over=600)
table(newData$Species)
```

-  Create six new cases for every member of the minority class.
-  Done through random interpolation between the case and its nearest neighbors.

SMOTE creates new cases
=======================
```{r, echo=FALSE}
table(data$Species)
table(newData$Species)
```

-  Use this training data for supervised learning techniques.

Visualizing SMOTE
=================
```{r, echo=FALSE}
par(mfrow=c(1,2))
plot(data[,1],data[,2],pch=19+as.integer(data[,3]),main='Original Data')
plot(newData[,1],newData[,2],pch=19+as.integer(newData[,3]),main="SMOTE'd Data")
```


Naive Bayes
============
type: sub-section

- *Naive Bayes* is a probabilistic classifier based on Bayes theorem.
-  Assumes independence between predictors, hence *naive*
-  Still fairly successful.
$$P(c|X_1, \ldots, X_p) = \frac{P(c)P(P(X_1, \ldots, X_p|c)}{P(X_1, \ldots, X_p)}$$

Implementation of Naive Bayes
=============================

-  Assume independence to simplify the numerator
$$P(c|X_1, \ldots, X_p) = \frac{P(c)\Pi_{i=1}^p P(X_i|c)}{P(X_1, \ldots, X_p)}$$
-  Estimate probabilities using the labeled test data.

R Implementations
===================

-  Naive Bayes is implemented in many packages.
-  Requires you specify the:
  -  Model
  -  Training data
  -  Predictors
-  Evaluation
  -  Based on hold-out routines

Using Naive Bayes
=================
```{r}
nb <- function(train,test) {
  require(e1071,quietly=T)
  sup <- which(train$Insp != 'unkn')
  data <- train[sup,c('ID','Prod','Uprice','Insp')]
  data$Insp <- factor(data$Insp,levels=c('ok','fraud'))
  model <- naiveBayes(Insp ~ .,data)
  preds <- predict(model,test[,c('ID','Prod','Uprice','Insp')],type='raw')
  return(list(rankOrder=order(preds[,'fraud'],decreasing=T),
              rankScore=preds[,'fraud'])
         )
}
```

Evaluation of Naive Bayes
==========================
```{r}
ho.nb <- function(form, train, test, ...) {
  res <- nb(train,test)
  structure(evalOutlierRanking(test,res$rankOrder,...),
            itInfo=list(preds=res$rankScore,
                        trues=ifelse(test$Insp=='fraud',1,0)
                       )
           )
}
```
```{r, echo=FALSE}
#nb.res <- holdOut(learner('ho.nb',
#                          pars=list(Threshold=0.1,
#                                    statsProds=globalStats)),
#                  dataset(Insp ~ .,sales),
#                  hldSettings(3,0.3,1234,T),
#                  itsInfo=TRUE
#                  )
load("NBresults.Rdata")
```

Results
========


```{r}
summary(nb.res)
```


```{r, echo=FALSE}
# ho.ORh <- function(form, train, test, ...) {
#   #require(dprep,quietly=T)
#   ntr <- nrow(train)
#   all <- rbind(train,test)
#   N <- nrow(all)
#   ups <- split(all$Uprice,all$Prod)
#   r <- list(length=ups)
#   for(u in seq(along=ups)) 
#     r[[u]] <- if (NROW(ups[[u]]) > 3) 
#                  outliers.ranking(ups[[u]])$prob.outliers 
#               else if (NROW(ups[[u]])) rep(0,NROW(ups[[u]])) 
#               else NULL
#   all$lof <- vector(length=N)
#   split(all$lof,all$Prod) <- r
#   all$lof[which(!(is.infinite(all$lof) | is.nan(all$lof)))] <- 
#     SoftMax(all$lof[which(!(is.infinite(all$lof) | is.nan(all$lof)))])
#   structure(evalOutlierRanking(test,order(all[(ntr+1):N,'lof'],
#                                            decreasing=T),...),
#             itInfo=list(preds=all[(ntr+1):N,'lof'],
#                         trues=ifelse(test$Insp=='fraud',1,0))
#            )
# }
# 
# 
# orh.res <- holdOut(learner('ho.ORh',
#                           pars=list(Threshold=0.1,
#                                     statsProds=globalStats)),
#                   dataset(Insp ~ .,sales),
#                   hldSettings(3,0.3,1234,T),
#                   itsInfo=TRUE
#                   )
# info <- attr(orh.res,'itsInfo')
# PTs.orh <- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)),
#                 )

```
Now, compare against ORh
========================

```{r, echo=FALSE}
par(mfrow=c(1,2))
info <- attr(nb.res,'itsInfo')
PTs.nb <- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)),
                 c(1,3,2)
                )
PRcurve(PTs.nb[,,1],PTs.nb[,,2],
        main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1),
        avg='vertical')
#PRcurve(PTs.orh[,,1],PTs.orh[,,2],
#        add=T,lty=1,col='grey',
#        avg='vertical')        
legend('topright',c('NaiveBayes','ORh'),
       lty=1,col=c('black','grey'))
CRchart(PTs.nb[,,1],PTs.nb[,,2],
        main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1),
        avg='vertical')
#CRchart(PTs.orh[,,1],PTs.orh[,,2],
#        add=T,lty=1,col='grey',
#        avg='vertical')        
legend('bottomright',c('NaiveBayes','ORh'),
       lty=1,col=c('black','grey'))
```

Why is it worse?
=================

Why is it worse?
================

-  Class imbalance?
-  Try to apply SMOTE

```{r, echo=FALSE}
# nb.s <- function(train,test) {
#   require(e1071,quietly=T)
#   sup <- which(train$Insp != 'unkn')
#   data <- train[sup,c('ID','Prod','Uprice','Insp')]
#   data$Insp <- factor(data$Insp,levels=c('ok','fraud'))
#   newData <- SMOTE(Insp ~ .,data,perc.over=700)
#   model <- naiveBayes(Insp ~ .,newData)
#   preds <- predict(model,test[,c('ID','Prod','Uprice','Insp')],type='raw')
#   return(list(rankOrder=order(preds[,'fraud'],decreasing=T),
#               rankScore=preds[,'fraud'])
#          )
# }
# 
# 
# ho.nbs <- function(form, train, test, ...) {
#   res <- nb.s(train,test)
#   structure(evalOutlierRanking(test,res$rankOrder,...),
#             itInfo=list(preds=res$rankScore,
#                         trues=ifelse(test$Insp=='fraud',1,0)
#                        )
#            )
# }
# nbs.res <- holdOut(learner('ho.nbs',
#                           pars=list(Threshold=0.1,
#                                     statsProds=globalStats)),
#                   dataset(Insp ~ .,sales),
#                   hldSettings(3,0.3,1234,T),
#                   itsInfo=TRUE
#                   )
load("NBSresults.Rdata")
```


Results with SMOTE
==================
```{r}
summary(nbs.res)
```

Comparison plot
===============
```{r, echo=FALSE}
par(mfrow=c(1,2))
info <- attr(nbs.res,'itsInfo')
PTs.nbs <- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)),
                 c(1,3,2)
                )
PRcurve(PTs.nb[,,1],PTs.nb[,,2],
        main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1),
        avg='vertical')
PRcurve(PTs.nbs[,,1],PTs.nbs[,,2],
        add=T,lty=2,
        avg='vertical')
#PRcurve(PTs.orh[,,1],PTs.orh[,,2],
#        add=T,lty=1,col='grey',
#        avg='vertical')        
legend('topright',c('NaiveBayes','smoteNaiveBayes'),
       lty=c(1,2),col=c('black','grey'))
CRchart(PTs.nb[,,1],PTs.nb[,,2],
        main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1),
        avg='vertical')
CRchart(PTs.nbs[,,1],PTs.nbs[,,2],
        add=T,lty=2,
        avg='vertical')
#CRchart(PTs.orh[,,1],PTs.orh[,,2],
#        add=T,lty=1,col='grey',
#        avg='vertical')        
legend('bottomright',c('NaiveBayes','smoteNaiveBayes'),
       lty=c(1,2),col=c('black','grey'))
```

Notes on Naive Bayes
=====================

-  Does not seem to work well here.
-  In the unsupervised methods, we split the model construction by product, but we do not have as much inspected data to work with here.


AdaBoost
==========

-  Learning algorithm that is part of the class of *ensemble models*.
-  *Ensemble models* are those that are formed by a set of base models that contribute to the final prediction.
-  *Boosting* is a general method that improves performance of a base that is better than a random classifier.
-  *AdaBoost* uses adaptive boosting to obtain the set of base models.

AdaBoost development
=====================

- Base models are generated sequentially.
- Each round weighs data by increasing weights on data that was incorrectly classified by the previous model.
- Effectively creates new training data for each round.
-  Since later rounds are forced to learn how to handle cases that were misclassified earlier, the resulting ensemble should be more accurate with them.

AdaBoost example
================
```{r, echo=FALSE}
library(RWeka)
data(iris)
idx<-sample(150, 100)
model <- AdaBoostM1(Species ~ ., iris[idx,],
                    control=Weka_control(I=100))
preds <- predict(model, iris[-idx,])
table(preds, iris[-idx, 'Species'])
```

And obtain probabilistic predictions
====================================
```{r}
prob.preds <- predict(model,iris[-idx,], type='probability')
head(prob.preds)
```

Apply to transactions set
=========================
```{r, echo=FALSE}
# ab <- function(train,test) {
#   require(RWeka,quietly=T)
#   sup <- which(train$Insp != 'unkn')
#   data <- train[sup,c('ID','Prod','Uprice','Insp')]
#   data$Insp <- factor(data$Insp,levels=c('ok','fraud'))
#   model <- AdaBoostM1(Insp ~ .,data,
#                       control=Weka_control(I=100))
#   preds <- predict(model,test[,c('ID','Prod','Uprice','Insp')],
#                    type='probability')
#   return(list(rankOrder=order(preds[,'fraud'],decreasing=T),
#               rankScore=preds[,'fraud'])
#          )
# }
# 
# 
# ho.ab <- function(form, train, test, ...) {
#   res <- ab(train,test)
#   structure(evalOutlierRanking(test,res$rankOrder,...),
#             itInfo=list(preds=res$rankScore,
#                         trues=ifelse(test$Insp=='fraud',1,0)
#                        )
#            )
# }
# 
# 
# ab.res <- holdOut(learner('ho.ab',
#                           pars=list(Threshold=0.1,
#                                     statsProds=globalStats)),
#                   dataset(Insp ~ .,sales),
#                   hldSettings(3,0.3,1234,T),
#                   itsInfo=TRUE
#                   )
load("ABresults.Rdata")
```
Look at the result summary
==========================
```{r}
summary(ab.res)
```

And plot AdaBoost results
=========================
```{r, echo=FALSE}
par(mfrow=c(1,2))
info <- attr(ab.res,'itsInfo')
PTs.ab <- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)),
                c(1,3,2)
                )
PRcurve(PTs.nb[,,1],PTs.nb[,,2],
        main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1),
        avg='vertical')
#PRcurve(PTs.orh[,,1],PTs.orh[,,2],
#        add=T,lty=1,col='grey',
#        avg='vertical')        
PRcurve(PTs.ab[,,1],PTs.ab[,,2],
        add=T,lty=2,
        avg='vertical')        
legend('topright',c('NaiveBayes','AdaBoostM1'),
       lty=c(1,2),col=c('black','black'))
CRchart(PTs.nb[,,1],PTs.nb[,,2],
        main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1),
        avg='vertical')
#CRchart(PTs.orh[,,1],PTs.orh[,,2],
#        add=T,lty=1,col='grey',
#        avg='vertical')        
CRchart(PTs.ab[,,1],PTs.ab[,,2],
        add=T,lty=2,
        avg='vertical')        
legend('bottomright',c('NaiveBayes','AdaBoostM1'),
       lty=c(1,2),col=c('black','black'))
```
