Review and data cleaning
========================================================
author: IE2064 Data Science
date: February 2014

Review
========================================================
type: section

Review
=======

-  Exploratory data analysis
-  Text processing
-  Unsupervised learning
-  Supervised learning

EDA
=====

-  Data munging
-  Visualization
-  Data summary

Visualization
==============

-  Types of plots
-  Visual cues
-  Combinations of visual cues

Qualities of big data
=====================
left:50%

- Volume
- Velocity
- Variety

***

![3Vs of Big data](resources/BigData3vs.png)

Descriptive models
==================
type: sub-section

Text processing
================

-  Preprocessing
-  Term document matrix
-  Associate terms
-  Cluster like documents
-  Summarize texts
-  Categorize texts

Popularity modeling
===================

-  Identify theoretical properties of process.
-  Use statistical tests for significant differences.

Frequent pattern mining
========================

-  Association rules
  -  Identify items that happen together
-  Term Frequency - Inverse Document Frequency
-  Lift as a measure of performance



Predictive models
==================
type: sub-section

- Regression
- Classification
- Clustering
- Dimensionality Reduction

Machine Learning
================
![Branches of Machine Learning](resources/ml_map.png)

Regression
==========

-  Used for making predictions
-  Linear model 
-  Model evaluation
  - *R*-squared
  - Cross validation
  - Mean squared error

Recursive partitioning
======================

-  Predict a value by using a hierarchy of logical (TRUE/FALSE) tests on some explanatory variables.
-  Creates a classification tree, prediction based on each class.

Supervised Learning
===================
type: sub-section

Time series
============

- Working with time series
- Working with predictive models
- Stock price data

Supervised learning methods
============================

-  Support Vector Machines
-  Artificial Neural Networks
-  Multivariate Adaptive Regression Splines

Support vector machines
=======================

-  Focus on classification problems.
-  Train on a set of data, then test results on a new set of data.
-  Goal is to create a geometric boundary that divides data into classes.
-  Input data is mapped into a *kernel* which is run on each vector represented in the input.

Artificial Neural Networks
=========================

- A neural network consists of an interconnected group of artificial neurons, and it processes information using a connectionist approach to computation. 
-  In most cases an ANN is an adaptive system that changes its structure based on external or internal information that flows through the network during the learning phase. 
-  Think of it as a non-linear extension of multiple logistic regression for classification problems.

nnet package
==========================

-  *Feed forward neural nets* is implemented in the R `nnet` package.
-  Formed by a set of computing units linked to each other.
  1. Linear combination of inputs.
  2. Non-linear computation to obtain output value.
  3. Result fed to other neurons in the network.
  4. Each neuron connection has a weight.
  
Support Vector Machines
=======================

-  A classifier based on a maximal margin classifier.
-  Determine the optimal separating hyperplane.
-  Hyperplane that separates the data that is farthest from the training observations.

SVM Hyperplane
==============

SVM finds the hyperplane that maximizes the separation between cases belonging to different classes.

![SVM plane](resources/svmplane.png)

Clustering and classification
=============================

-  Naive Bayes
-  *Adaboost* classifiers
-  Random Forests
-  *k*-Nearest Neighbors

Evaluation
===========

-  Cross-validation
-  Precision/Recall and cumulative recall curves.
-  Hold-out experiments

Detecting fraudulent transactions
=================================
type: section

Detecting fraudulent transactions
=================================

-  Goal:  Detect unusual events.
-  Example:  Salesperson transaction reports
-  What makes this hard?
-  What type of problem is this?

Data mining tasks
=================

-  Outlier detection
-  Classification
-  Semi-supervised learning

Outlier detection
=================
type: sub-section

Fraud detection
===============

-  Frauds are usually associated with unusual observations that are deviations from the norm.
-  *outlier* - An observation which deviates so much from other observations as to arouse suspicions that it was generated by a different mechanism.

Data set
========

-  Transactions reported by sales people.
-  Salespeople have flexibility in setting selling price.
-  Report transactions on a regular basis.
-  Question: How to verify transactions and efficiently use fraud investigation resources?

Data elements
=============

- ID – ID of the salesman.
- Prod – ID of the sold product.
- Quant – the number of reported sold units.
- Val – the reported total monetary value of the sale.
- Insp – `ok` if the transaction was
inspected and considered valid by the company, `fraud` if the transaction
was found to be fraudulent, and `unkn` if the transaction was not inspected
at all by the company.

Look at the data
================

```{r salesdata, echo=FALSE}
library(DMwR)
data(sales)
head(sales)
```

Summary of data
===============

<small>
```{r salessummary, echo=FALSE}
summary(sales)
```
</small>

Look at unique products and salespeople
=======================================
```{r}
print(nlevels(sales$ID))
print(nlevels(sales$Prod))
```

Look at the NAs, do they occur together?
========================================

```{r}
length(which(is.na(sales$Quant) & is.na(sales$Val)))
```

Now, look at the proportion of frauds of those inspected
=========================================================

```{r}
table(sales$Insp)/nrow(sales) * 100
```

Now look at the transactions
============================
```{r, echo=FALSE}
totS <- table(sales$ID)
totP <- table(sales$Prod)
barplot(totS,main='Transactions per salespeople',names.arg='',xlab='Salespeople',ylab='Amount')
```
***
```{r, echo=FALSE}
barplot(totP,main='Transactions per product',names.arg='',xlab='Products',ylab='Amount')
```

Look at the data elements that have lots of variability
==========================================================

-  Create a new measure of price per unit
<small>
```{r}
sales$Uprice <- sales$Val/sales$Quant
summary(sales$Uprice)
```
</small>

There are lots of products, look at the least and most expensive
================================================================

```{r}
upp <- aggregate(sales$Uprice,list(sales$Prod),median,na.rm=T)
topP <- sapply(c(T,F),function(o) 
               upp[order(upp[,2],decreasing=o)[1:5],1])
colnames(topP) <- c('Expensive','Cheap')
topP
```

Now, look at the distribution of unit price for these items
============================================================
```{r}
tops <- sales[sales$Prod %in% topP[1,],c('Prod','Uprice')]
tops$Prod <- factor(tops$Prod)
boxplot(Uprice ~ Prod,data=tops,ylab='Unit price',log="y")
```

Change focus to salespeople in terms of value 
==============================================

```{r}
vs <- aggregate(sales$Val,list(sales$ID),sum,na.rm=T)
scoresSs <- sapply(c(T,F),function(o) 
                   vs[order(vs$x,decreasing=o)[1:5],1])
colnames(scoresSs) <- c('Most','Least')
scoresSs
```

Fraction of value accounted for by top salespeople
=====================================================

```{r}
print(sum(vs[order(vs$x,decreasing=T)[1:100],2])/sum(sales$Val,na.rm=T)*100)
print(sum(vs[order(vs$x,decreasing=F)[1:2000],2])/sum(sales$Val,na.rm=T)*100)
```

Top products in terms of volume
==============================================

```{r}
qs <- aggregate(sales$Quant,list(sales$Prod),sum,na.rm=T)
scoresPs <- sapply(c(T,F),function(o) 
                   qs[order(qs$x,decreasing=o)[1:5],1])
colnames(scoresPs) <- c('Most','Least')
scoresPs
```

And the fraction accounted for by the top products
=========================================================
```{r}
sum(as.double(qs[order(qs$x,decreasing=T)[1:100],2]))/
  sum(as.double(sales$Quant),na.rm=T)*100
sum(as.double(qs[order(qs$x,decreasing=F)[1:4000],2]))/
  sum(as.double(sales$Quant),na.rm=T)*100
```

First thoughts
==============

-  The top 100 products represent 75% of volume.
-  Bottom 1000 products represent less than 10%.
-  What should the distribution of price per unit of a given product be?
-  Remember what the process of setting the price is.

Outliers
=========

Using the box and whisker plot
-  *Interquartile range* (IQR) - the distance between the 25th (Q1) and 75th (Q3) quantiles.
-  Top whisker = $Q3 + 1.5 IQR$
-  Bottom whisker = $Q1 - 1.5 IQR$
-  Outliers are points outside the whiskers.

Look for outliers
=================

```{r}
out <- tapply(sales$Uprice,list(Prod=sales$Prod), 
              function(x) length(boxplot.stats(x)$out))
out[order(out,decreasing=T)[1:10]]
```
```{r}
sum(out)/nrow(sales)*100
```

Data munging
============

-  What to do with unknown values?

1. remove the cases, 
2. fill in the unknowns using some strategy, or 
3. use tools that handle these types of values.

-  We are looking for fraud, so we don't want to use tools that manipulate the data.

```{r, echo=FALSE}
totS <- table(sales$ID)
totP <- table(sales$Prod)
```

First thought, remove unknowns
===============================

-  Check to see if this causes problems by certain salespeople being overly represented.

```{r}
nas <- sales[which(is.na(sales$Quant) & is.na(sales$Val)),c('ID','Prod')]
propS <- 100*table(nas$ID)/totS
propS[order(propS,decreasing=T)[1:10]]
```

-  Not too bad.

Check products for NA
=====================
```{r}
propP <- 100*table(nas$Prod)/totP
propP[order(propP,decreasing=T)[1:10]]
```

-  Some of these are real high.
-  We could check to see if in general these had the same distributions as some other products. (yes) 
-  If so, we could combine products.

Try removing anything with NA in both value and quantity
========================================================

```{r}
sales1 <- sales[-which(is.na(sales$Quant) & is.na(sales$Val)),]
nnasQp <- tapply(sales1$Quant,list(sales1$Prod),
                 function(x) sum(is.na(x)))
propNAsQp <- nnasQp/table(sales1$Prod)
propNAsQp[order(propNAsQp,decreasing=T)[1:10]]
```

Note the 1.0000. Let's remove these


Remove the products with no transaction information
===================================================
```{r}
sales2 <- sales1[!sales1$Prod %in% c('p2442','p2443'),]
nlevels(sales2$Prod)
sales2$Prod <- factor(sales2$Prod)
nlevels(sales2$Prod)
```

Check salespeople for no quantity information
=============================================

```{r}
nnasQs <- tapply(sales2$Quant, list(sales2$ID), function(x) sum(is.na(x)))
propNAsQs <- nnasQs/table(sales2$ID)
propNAsQs[order(propNAsQs, decreasing = T)[1:10]]
```
- Should we remove the sales people without quantity information?

Look for transactions without quantity information 
===================================================
```{r}
nnasVp <- tapply(sales$Val,list(sales$Prod),
                 function(x) sum(is.na(x)))
propNAsVp <- nnasVp/table(sales$Prod)
propNAsVp[order(propNAsVp,decreasing=T)[1:10]]
```

Assume that there is a 'typical unit price'
===========================================

```{r}
tPrice <- tapply(sales2[sales2$Insp != 'fraud','Uprice'],list(sales2[sales2$Insp != 'fraud','Prod']),median,na.rm=T)
```

Fill in missing values
======================

-  Since there are no longer any transactions with both `Quant` and `Val` both missing, use them to fill in the missing values.
-  Recalculate unit prices
```{r}
noQuant <- which(is.na(sales2$Quant))
sales2[noQuant,'Quant'] <- ceiling(sales2[noQuant,'Val'] /
                       tPrice[sales2[noQuant,'Prod']])
noVal <- which(is.na(sales2$Val))
sales2[noVal,'Val'] <- sales2[noVal,'Quant'] *
                   tPrice[sales2[noVal,'Prod']]
sales2$Uprice <- sales2$Val/sales2$Quant
summary(sales2$Uprice)
```
